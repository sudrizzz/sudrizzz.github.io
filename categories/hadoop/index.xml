<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hadoop on Anthony's blog</title><link>https://jinggqu.github.io/categories/hadoop/</link><description>Recent content in Hadoop on Anthony's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 23 Oct 2020 20:00:00 +0800</lastBuildDate><atom:link href="https://jinggqu.github.io/categories/hadoop/index.xml" rel="self" type="application/rss+xml"/><item><title>Spark 分布式内存计算框架</title><link>https://jinggqu.github.io/posts/spark-distributed-programming/</link><pubDate>Fri, 23 Oct 2020 20:00:00 +0800</pubDate><guid>https://jinggqu.github.io/posts/spark-distributed-programming/</guid><description>&lt;h1 id="spark-简介">Spark 简介&lt;/h1>
&lt;p>Spark 是一种基于内存的、用以实现高效集群计算的平台。准确地讲，Spark 是一个大数据并行计算框架，是对广泛使用的 MapReduce 计算模型的扩展。Spark 有着自己的生态系统，但同时兼容
HDFS、Hive 等分布式存储系统，可以完美融入 Hadoop 的生态圈中，代替 MapReduce 去执行更为高效的分布式计算。两者的区别在于：基于 MapReduce 的计算引擎通常会将中间结果输出到磁盘上进行存储和容错；而 Spark 则是将中间结果尽量保存在内存中以减少底层存储系统的 I/O，以提高计算速度。&lt;/p>
&lt;h1 id="spark-编程模型">Spark 编程模型&lt;/h1>
&lt;h2 id="核心数据结构-rdd">核心数据结构 RDD&lt;/h2>
&lt;p>Spark 将数据抽象成弹性分布式数据集（Resilient Distributed Dataset, RDD），RDD 实际是分布在集群多个节点上数据的集合，通过操作 RDD 对象来并行化操作集群上的分布式数据。&lt;/p>
&lt;p>RDD 有两种创建方式:&lt;/p>
&lt;ol>
&lt;li>并行化驱动程序中已有的原生集合;&lt;/li>
&lt;li>引用 HDFS、HBase 等外部存储系统上的数据集。&lt;/li>
&lt;/ol>
&lt;p>RDD 可以缓存在内存中，每次对 RDD 操作的结果都可以放到内存中，下一次操作时可直接从内存中读取，相对于 MapReduce,它省去了大量的磁盘 I/O 操作。另外，持久化的 RDD 能够在错误中自动恢复，如果某部分 RDD 丢失，Spark 会自动重算丢失的部分。&lt;/p>
&lt;h2 id="rdd-上的操作">RDD 上的操作&lt;/h2>
&lt;p>从相关数据源获取初始数据形成初始 RDD 后，需要根据应用的需求对得到的初始 RDD 进行必要的处理，来获取满足需求的数据内容，从而对中间数据进行计算加工，得到最终的数据。&lt;/p>
&lt;p>RDD 支持两种操作，一种是转换（Transformation）操作，另一种是行动（Action）操作。&lt;/p>
&lt;h3 id="转换transformation操作">转换（Transformation）操作&lt;/h3>
&lt;p>转换操作即将一个 RDD 转换为一个新的 RDD。值得注意的是，转换操作是惰性的，这就意味着对 RDD 调用某种转换操作时，操作并不会立即执行，而是 Spark 在内部记录下所要求执行的操作的相关信息，当在行动操作中需要用到这些转换出来的 RDD 时才会被计算，下表所示为基本的转换操作。通过转换操作，可以从已有的 RDD 生成出新的 RDD, Spark 使用谱系（Lineage）记录新旧 RDD 之间的依赖关系，一旦持久化的 RDD 丢失部分数据时，Spark 能通过谱系图重新计算丢失的数据。&lt;/p>
&lt;blockquote>
&lt;p>输入数据为 {1, 2, 3, 3}&lt;/p>
&lt;/blockquote>
&lt;style>
table th:first-of-type {
width: 10%;
}
table th:nth-of-type(2) {
width: 40%;
}
table th:nth-of-type(3) {
width: 20%;
}
table th:nth-of-type(4) {
width: 30%;
}
&lt;/style>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>函数名&lt;/th>
&lt;th>目的&lt;/th>
&lt;th>示例&lt;/th>
&lt;th style="text-align:center">结果&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>map()&lt;/td>
&lt;td>将数据集中的每个元素经过用户自定义的函数转换形成一个新的 RDD&lt;/td>
&lt;td>rdd.map(x =&amp;gt; x * 2)&lt;/td>
&lt;td style="text-align:center">{2, 4, 6, 6}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>flatMap()&lt;/td>
&lt;td>与 map() 类似，但每个元素输入项都可以被映射到 0 个或多个的输出项，最终将结果“扁平化“后输出&lt;/td>
&lt;td>rdd.flatMap(x =&amp;gt; (1 to x))&lt;/td>
&lt;td style="text-align:center">{1, 1, 2, 1, 2, 3, 1, 2, 3, 3}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>filter()&lt;/td>
&lt;td>对 RDD 元素进行过滤，把经过指定函数后返回值为 true 的元素组成一个新的 RDD&lt;/td>
&lt;td>rdd.filter(x =&amp;gt; (x != 3))&lt;/td>
&lt;td style="text-align:center">{1, 2}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>distinct()&lt;/td>
&lt;td>对数据进行去重，返回一个新的 RDD&lt;/td>
&lt;td>rdd.distinct()&lt;/td>
&lt;td style="text-align:center">{1, 2, 3}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sample(withReplacement, fraction, seed)&lt;/td>
&lt;td>以指定的随机种子随机抽样出数量为 fraction 的数据，withReplacement 表示是抽出的数据是否放回，true 为有放回的抽样，false 为无放回的抽样&lt;/td>
&lt;td>rdd.sample(true,0.5,3)&lt;/td>
&lt;td style="text-align:center">非确定的&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="行动action操作">行动（Action）操作&lt;/h3>
&lt;p>行动操作会触发 Spark 提交作业，对 RDD 进行实际的计算，并将最终求得的结果返回到驱动器程序，或者写入外部存储系统中。由于行动操作会得到一个结果，所以 Spark 会强制对 RDD 的转换操作进行求值，下表所示为基本的行动操作。&lt;/p>
&lt;blockquote>
&lt;p>输入数据为 {1, 2, 3, 3}&lt;/p>
&lt;/blockquote>
&lt;style>
table th:first-of-type {
width: 20%;
}
table th:nth-of-type(2) {
width: 35%;
}
table th:nth-of-type(3) {
width: 25%;
}
table th:nth-of-type(4) {
width: 20%;
}
&lt;/style>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>函数名&lt;/th>
&lt;th>目的&lt;/th>
&lt;th>示例&lt;/th>
&lt;th style="text-align:center">结果&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>collect()&lt;/td>
&lt;td>返回 RDD 中的所有元素&lt;/td>
&lt;td>rdd.collect()&lt;/td>
&lt;td style="text-align:center">{1, 2, 3, 3}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>count()&lt;/td>
&lt;td>返回 RDD 中元素的个数&lt;/td>
&lt;td>rdd.count()&lt;/td>
&lt;td style="text-align:center">4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>countByValue()&lt;/td>
&lt;td>返回 RDD 中各元素出现的次数&lt;/td>
&lt;td>rdd.countByValue()&lt;/td>
&lt;td style="text-align:center">{(1, 1), (2, 1), (3, 2)}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>take(n)&lt;/td>
&lt;td>从 RDD 中返回 n 个元素（任意位置）&lt;/td>
&lt;td>rdd.take(2)&lt;/td>
&lt;td style="text-align:center">{2, 3}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>top(n)&lt;/td>
&lt;td>从 RDD 中返回&lt;strong>前&lt;/strong> n 个元素&lt;/td>
&lt;td>rdd.top(2)&lt;/td>
&lt;td style="text-align:center">{1, 2}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>reduce(func)&lt;/td>
&lt;td>并行整合 RDD 中的所有数据&lt;/td>
&lt;td>rdd.reduce((x, y) =&amp;gt; x + y)&lt;/td>
&lt;td style="text-align:center">9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>fold(zero)(func)&lt;/td>
&lt;td>与 reduce() 类似，但需要提供初始值。加法的默认是 0；乘法的默认是 1&lt;/td>
&lt;td>rdd.fold(1)((x, y) =&amp;gt; x + y)&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>aggregate()&lt;/td>
&lt;td>与 reduce() 类似，但通常返回不同类型的函数&lt;/td>
&lt;td>rdd.aggregate((0, 0))&lt;br/>((x, y) =&amp;gt; &lt;br/>(x._1 + y, x._2 + 1), &lt;br/>(x, y) =&amp;gt; &lt;br/>(x._1 + y._1, x._2 + y._2))&lt;/td>
&lt;td style="text-align:center">(9, 4)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>foreach(func)&lt;/td>
&lt;td>对 RDD 中的每个元素使用给定的函数&lt;/td>
&lt;td>rdd.foreach(func)&lt;/td>
&lt;td style="text-align:center">无&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="示例">示例&lt;/h1>
&lt;p>以下两个示例的数据集与源代码均可以在下述链接中进行下载
&lt;a class="link" href="https://github.com/jinggqu/BigDataTechnologyFoundation_SourceCodeAndDataSet/tree/main/ch08" target="_blank" rel="noopener"
>https://github.com/jinggqu/BigDataTechnologyFoundation_SourceCodeAndDataSet/tree/main/ch08&lt;/a>&lt;/p>
&lt;h2 id="一分词">一、分词&lt;/h2>
&lt;p>WordCount（单词统计程序）是大数据领域经典的例子，与 Hadoop 实现的 WordCount 程序相比，Spark 实现的版本要显得更加简洁。&lt;/p>
&lt;h3 id="从-mapreduce-到-spark">从 MapReduce 到 Spark&lt;/h3>
&lt;p>在经典的计算框架 MapReduce 中，问题会被拆成两个主要阶段: map 阶段和 reduce 阶段。对单词计数来说，MapReduce 程序从 HDFS 中读取一行字符串。在 map 阶段，将字符串分割成单词，并生成 &amp;lt;word, 1&amp;gt; 这样的键值对；在 reduce 阶段，将单词对应的计数值（初始为 1）全部累加起来，最后得到单词的总出现次数。&lt;/p>
&lt;p>在 Spark 中，并没有 map/reduce 这样的划分，而是以 RDD 的转换来呈现程序的逻辑。首先，Spark 程序将从 HDFS 中按行读取的文本作为初始 RDD（即集合的每一个元素都是一行字符串）；然后，通过 flatMap 操作将每一行字符串分割成单词，并收集起来作为新的单词 RDD；接着，使用 map 操作将每一个单词映射成 &amp;lt;word, 1&amp;gt;这样的键值对，转换成新的键值对 RDD；最后，通过 reduceByKey 操作将相同单词的计数值累加起来，得到单词的总出现次数。&lt;/p>
&lt;h3 id="java-实现">Java 实现&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.spark.SparkConf&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.spark.api.java.JavaPairRDD&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.spark.api.java.JavaRDD&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.spark.api.java.JavaSparkContext&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">scala.Tuple2&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">java.util.Arrays&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">java.util.List&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">java.util.regex.Pattern&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">class&lt;/span> &lt;span class="nc">SparkDemo&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">private&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">final&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Pattern&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">kSpace&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Pattern&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">compile&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">main&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">[]&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">SparkConf&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">conf&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SparkConf&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="na">setAppName&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;WordCount&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaSparkContext&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">sc&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">JavaSparkContext&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">conf&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaRDD&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">lines&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">sc&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">textFile&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="o">[&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">]&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="na">rdd&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="na">toJavaRDD&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaRDD&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">words&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">lines&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">flatMap&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Arrays&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">asList&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kSpace&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)).&lt;/span>&lt;span class="na">iterator&lt;/span>&lt;span class="p">());&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaPairRDD&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ones&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">words&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">mapToPair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Tuple2&lt;/span>&lt;span class="o">&amp;lt;&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaPairRDD&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">counts&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ones&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">reduceByKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Tuple2&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">counts&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">collect&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">for&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Tuple2&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuple&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tuple&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">_1&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34; : &amp;#34;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuple&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">_2&lt;/span>&lt;span class="p">());&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">sc&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">close&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="运行过程分析">运行过程分析&lt;/h3>
&lt;ol>
&lt;li>初始化
创建配置文件 SparkConf，这里仅设置应用名称；再创建 JavaSparkContext，在程序中主要通过 JavaSparkContext 来访问 Spark 集群；&lt;/li>
&lt;li>处理数据
&lt;ol>
&lt;li>根据参数使用 Spark.read().textFile() 方法按行读取输入文件，并转换成 RDD lines；&lt;/li>
&lt;li>使用 flatMap 操作将所有行按空格分割切割成词，并生成新的 RDD words；&lt;/li>
&lt;li>使用 map 操作( Java 中为 mapToPair )，将词映射成 &amp;lt;word, 1&amp;gt;键值对 RDD ones，其中 1 表示出现一次；&lt;/li>
&lt;li>使用 reduceByKey 操作将所有相同的 word 对应的计数累加起来，得到新的 RDD counts；&lt;/li>
&lt;li>使用 collect 操作将所有结果打印出来；&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>关闭 JavaSparkContext。&lt;/li>
&lt;/ol>
&lt;h3 id="执行">执行&lt;/h3>
&lt;p>将上述代码生成 Jar 包之后，将其放到服务器中，执行下面的命令即可开始运行。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">./bin/spark-submit --class SparkDemo ~/Documents/SparkDemo.jar ~/Documents/sample.txt
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中&lt;/p>
&lt;ul>
&lt;li>&amp;ndash;class SparkDemo 用来指定主类名&lt;/li>
&lt;li>~/Documents/SparkDemo.jar 指定 Jar 包路径&lt;/li>
&lt;li>~/Documents/sample.txt 指定测试文本路径&lt;/li>
&lt;/ul>
&lt;p>sample.txt 文本内容如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Your want text it even a text notes having wrong even about fake want or not even but. Language way contentwise just language contentwise recipes set start are. Recipes a words than with meeting days ?looks? even than is name story more story words generator anything gone. Having story but fairly random some adequate want it set has a kind looking having. Fantasy anything you looks just copy work text random sets even fake having. Piece some recipes repetitive adequate wrong wrong way options to repetitive working some dummy repetitive copy realistic you fake. Work or just fairly with is unrelated having language about set forever not game repetitive adequate now you looks it of even dummy now. That but design language unrelated copy you text placeholder has review those of with fake. Random to want the has gardening which business some realistic that and just work. Gardening you realistic kind and name looks about name words words way which some name.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">The that copy story realistic the adequate text meeting options game gone piece has options has name random. Days wrong set realistic design repetitive adequate review text your or but having start about right are story fairly fairly but to language sets adequate. But work want sets right kind some having contentwise fairly convincing language notes right name from but want realistic unrelated words. From about generator not looks or fairly copy has more. Forever from gone which or that having a with some having the work wrong generator design a fantasy way convincing. Working in dummy not now happily but to it of happily story want those kind looking right words business it generator language are. A you anything sets fake sets kind notes meeting having has in copy realistic is you. Copy or fairly set story.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Wrong and days not work of want piece options unrelated way random just just recipes. Your recipes gardening start fairly. Happily start game days want from in set meeting that forever random. Support has wrong than your language are random business a even design has. Way design dummy unrelated set generator game convincing. Text contentwise copy to of set kind notes a you ?looks? gone work. Way forever result you to not. Your your meeting generator way placeholder looking than has want in repetitive more kind start has you but language a. A than notes name story and a just days some with in options looking just not are want looks. Kind some from review even.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Some start random meeting recipes is a a ?looks? unrelated more the about but are dummy. Words review fake now kind of you meeting it design your. To just a about to. Not realistic name from with fake is. Work even business options fake wrong result notes want the more has dummy a notes random. Gone right repetitive fairly want now it want days review. Has notes want random name that random fantasy not unrelated in is dummy work work random game design now. Business result a and piece from working. Your some recipes copy sets are has kind story support fantasy has and some fantasy a which anything are the.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Language piece that kind copy right anything dummy a of copy which fantasy placeholder which the work are convincing random. Your gone way copy you copy are that game but looking gardening result is start text the words the a anything. Want piece set set fantasy generator sets a more are happily or ?looks? just the and sets not anything. Support to just start game work looks copy that in of but words placeholder support now fairly fake even now. Text adequate words not fairly looks from game that result name realistic or you fake working want. Kind you some looking of review has sets than want the way working has. Of fantasy gardening and kind just game those adequate your from or text are you story working happily. Business set way gardening more dummy want are you business ?looks? work to placeholder are design options sets having. Working from options work right not meeting story it is of which way fake meeting. Adequate story than words want the anything.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Language some gone random or just fairly gone which adequate sets having and adequate or text random from review. From unrelated those a start the ?looks? game business. With copy and which set kind game contentwise which anything the set story notes about or forever. Way anything work ?looks? a contentwise adequate and meeting. Options which realistic words it of to right game random way random your those and those anything some you notes gone gardening dummy than fake. But language just a your work with that set the. Are dummy business story not gardening start wrong fantasy fake and words having text which recipes your ?looks? wrong or. Generator fake than set looking text now forever more design ?looks? text but than has than wrong.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Way than fake gardening those a now it language but piece. A is even looks just result that which realistic gone are working right fake some. Which language wrong having with that looks.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>执行结果如下所示&lt;/p>
&lt;pre>&lt;code>right : 9
Fantasy : 1
review : 5
convincing : 2
is : 8
Business : 2
even : 1
Are : 1
even : 10
start : 10
// 此处省略数行
&lt;/code>&lt;/pre>
&lt;h2 id="二统计用户的视频上传数">二、统计用户的视频上传数&lt;/h2>
&lt;h3 id="场景分析">场景分析&lt;/h3>
&lt;p>接下来使用 Spark 来统计 Youtube 的测试数据集中每个用户的视频上传数量。稍加分析，会发现统计每个用户的视频数量其实与 WordCount 中统计每个单词出现的次数的逻辑几乎一致，区别在于处理 Youtube 测试数据集的格式略为复杂些。将给定的数据集按行划分，每行代表一条记录，除了视频类别这一字段中间有可能出现空格之外，其他的字段都是用空格分割。可以考虑使用正则表达式来匹配记录，并提取所需要的信息。&lt;/p>
&lt;p>在测试数据集中，假定每行所代表的视频都是唯一的，所以仅仅需要用户 ID 这一条信息。在提取到用户 ID 之后，可以像 WordCount 一样，组成 &amp;lt;ID, 1&amp;gt; 这样的用来计数的键值对，这步之后的逻辑便与 WordCount 相似了。&lt;/p>
&lt;h3 id="java-实现-1">Java 实现&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.spark.SparkConf&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.spark.api.java.JavaPairRDD&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.spark.api.java.JavaRDD&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.spark.api.java.JavaSparkContext&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">scala.Tuple2&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">java.util.ArrayList&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">java.util.List&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">java.util.regex.Matcher&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">java.util.regex.Pattern&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">class&lt;/span> &lt;span class="nc">SparkDemo&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">private&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">final&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Pattern&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">EXTRACT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Pattern&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">compile&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;(\\S+)\\s+(\\S+)\\s+(\\d+)\\s+(\\D+[a-zA-Z])\\s+(\\d+)\\s+(\\d+)\\s+(\\d+\\.?\\d*)\\s+(\\d+)\\s+(\\d+)\\s+(.*)&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">main&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">[]&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">SparkConf&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">conf&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SparkConf&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="na">setAppName&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;CountUploader&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaSparkContext&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">sc&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">JavaSparkContext&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">conf&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaRDD&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">lines&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">sc&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">textFile&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="o">[&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">]&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaRDD&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">filtered&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">lines&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">filter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">EXTRACT&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">matcher&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="na">matches&lt;/span>&lt;span class="p">());&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaPairRDD&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">records&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">filtered&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">mapToPair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">Matcher&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">EXTRACT&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">matcher&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kt">boolean&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">matches&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">return&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Tuple2&lt;/span>&lt;span class="o">&amp;lt;&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">group&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">group&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">});&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaPairRDD&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">groups&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">records&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">groupByKey&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="na">mapToPair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">list&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ArrayList&lt;/span>&lt;span class="o">&amp;lt;&amp;gt;&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">_2&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="na">forEach&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">list&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">return&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Tuple2&lt;/span>&lt;span class="o">&amp;lt;&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">_1&lt;/span>&lt;span class="p">(),&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">list&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">});&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c1">// 手动实现 sortBy 操作&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">JavaRDD&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Tuple2&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tops&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">groups&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">keyBy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">_2&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="na">size&lt;/span>&lt;span class="p">()).&lt;/span>&lt;span class="na">sortByKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="na">values&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Tuple2&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">topList&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tops&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">take&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">100&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">for&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Tuple2&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">topList&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;User: &amp;#34;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">_1&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34;, Number of videos: &amp;#34;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">_2&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="na">size&lt;/span>&lt;span class="p">());&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">sc&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">stop&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="执行-1">执行&lt;/h3>
&lt;p>将上述代码生成 Jar 包之后，将其放到服务器中，执行下面的命令即可开始运行。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">./bin/spark-submit --class SparkDemo ~/Documents/SparkDemo.jar ~/Documents/YoutubeDataSets.txt
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>执行结果如下所示&lt;/p>
&lt;pre>&lt;code>User: machinima, Number of videos: 21
User: hotforwords, Number of videos: 19
User: theevang1, Number of videos: 19
User: kushtv, Number of videos: 19
User: supermac18, Number of videos: 18
User: NBA, Number of videos: 18
User: somedia, Number of videos: 17
User: tokiohotelchannel, Number of videos: 17
User: AtheneWins, Number of videos: 16
User: davidisbetterthenyou, Number of videos: 16
// 此处省略数行
&lt;/code>&lt;/pre>
&lt;h1 id="参考文章">参考文章&lt;/h1>
&lt;ol>
&lt;li>&lt;a class="link" href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations" target="_blank" rel="noopener"
>RDD Operations&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/MOBIN/p/5373256.html" target="_blank" rel="noopener"
>Spark 函数详解系列之 RDD 基本转换&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://juejin.im/post/6844904147502759943" target="_blank" rel="noopener"
>Spark 教程之 RDD 操作-转换和执行（示例）&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://yxnchen.github.io/technique/Spark%E7%AC%94%E8%AE%B0-%E7%8E%A9%E8%BD%ACRDD%E6%93%8D%E4%BD%9C/" target="_blank" rel="noopener"
>Spark 笔记-玩转 RDD 操作&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://stackoverflow.com/questions/37018249/rdd-aggregate-in-spark" target="_blank" rel="noopener"
>RDD Aggregate in spark&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://dblab.xmu.edu.cn/blog/1327/" target="_blank" rel="noopener"
>利用开发工具 IntelliJ IDEA 编写 Spark 应用程序（Scala+Maven）&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>MapReduce 分布式编程</title><link>https://jinggqu.github.io/posts/mapreduce-distributed-programming/</link><pubDate>Sat, 17 Oct 2020 20:00:00 +0800</pubDate><guid>https://jinggqu.github.io/posts/mapreduce-distributed-programming/</guid><description>&lt;h1 id="词频统计程序示例">词频统计程序示例&lt;/h1>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20201014180000.png"
loading="lazy"
alt="20201014180000"
>&lt;/p>
&lt;p>假设将一个英文文本大文件作为输入，统计文件中单词出现的频数。最基本的操作是把输入文件的每一行传递给 map 函数完成对单词的拆分并输出中间结果，中间结果为 &amp;lt;word, 1&amp;gt; 的形式， 表示程序对一个单词，都对应一个计数 1。使用 reduce 函数收集 map 函数的结果作为输入值，并生成最终 &amp;lt;word, count&amp;gt; 形式的结果，完成对每个单词的词频统计。它们对应 MapReduce 处理数据流程如上图所示。&lt;/p>
&lt;h1 id="mapreduce-程序的运行过程">MapReduce 程序的运行过程&lt;/h1>
&lt;p>如图所示，MapReduce 运行阶段数据传递经过输入文件、Map 阶段、中间文件、 Reduce 阶段、输出文件五个阶段，用户程序只与 Map 阶段和 Reduce 阶段的 Worker 直接相关，其他事情由 Hadoop 平台根据设置自行完成。&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20201014190000.png"
loading="lazy"
alt="20201014190000"
>&lt;/p>
&lt;p>从用户程序 User Program 开始，用户程序 User Program 链接了 MapReduce 库，实现了最基本的 map 函数和 reduce 函数。&lt;/p>
&lt;ol>
&lt;li>MapReduce 库先把 User Program 的输入文件划分为 M 份，如上图左方所示，将数据分成了分片 0~4，每一份通常为 16MB~64MB；然后使用 fork 将用户进程复制到集群内其他机器上。&lt;/li>
&lt;li>User Program 的副本中有一个 Master 副本和多个 Worker 副本。Master 是负责调度的，为空闲 Worker 分配 Map 作业或者 Reduce 作业。&lt;/li>
&lt;li>被分配了 Map 作业的 Worker，开始读取对应分片的输入数据, Map 作业数量与输入文件划分数 M 相同，并与分片一一对应; Map 作业将输入数据转化为键值对表示形式并传递给 map 函数，map 函数产生的中间键值对被缓存在内存中。&lt;/li>
&lt;li>缓存的中间键值对会被定期写入本地磁盘，而且被分为 R 个区（R 的大小是由用户定义的），每个区会对应一个 Reduce 作业；这些中间键值对的位置会被通报给 Master, Master 负责将信息转发给 Reduce Worker。&lt;/li>
&lt;li>Master 通知分配了 Reduce 作业的 Worker 负责数据分区，Reduce Worker 读取键值对数据并依据键排序，使相同键的键值对聚集在一起。同一个分区可能存在多个键的键值对，而 reduce 函数的一次调用的键值是唯一的， 所以必须进行排序处理。&lt;/li>
&lt;li>Reduce Worker 遍历排序后的中间键值对，对于每个唯一的键，都将键与关联的值传递给 reduce 函数，reduce 函数产生的输出会写回到数据分区的输出文件中。&lt;/li>
&lt;li>当所有的 Map 和 Reduce 作业都完成了，Master 唤醒 User Program，MapReduce 函数调用返回 User Program。&lt;/li>
&lt;/ol>
&lt;p>执行完毕后，MapReduce 的输出放在 R 个分区的输出文件中，即每个 Reduce 作业分别对应一个输出文件。用户可将这 R 个文件作为输入交给另一个 MapReduce 程序处理，而不需要主动合并这 R 个文件。在 MapReduce 计算过程中，输入数据来自分布式文件系统，中间数据放在本地文件系统，最终输出数据写入分布式文件系统。&lt;/p>
&lt;p>必须指出 Map 或 Reduce 作业和 map 或 reduce 函数存在以下几个区别:&lt;/p>
&lt;ol>
&lt;li>Map 或 Reduce 作业是从计算框架的角度来认识的，而 map 或 reduce 函数是需要程序员编写代码完成的，并在运行过程中被对用 Map 或 Reduce 作业调度;&lt;/li>
&lt;li>Map 作业处理一个输入数据的分片，可能需要多次调用 map 函数来处理输入的键值对;&lt;/li>
&lt;li>Reduce 作业处理一个分区的中间键值对，期间要对每个不同的键调用一次 reduce 函数，一个 Reduce 作业最终对应一个输出文件。&lt;/li>
&lt;/ol>
&lt;h1 id="经典-mapreduce-任务调度模型">经典 MapReduce 任务调度模型&lt;/h1>
&lt;p>经典 MapReduce 任务调度模型采用主从结构（Master/Slave），包含四个组成部分：Client、JobTracker、TaskTracker、Task。支撑 MapReduce 计算框架的是 JobTracker 和 TaskTracker 两类后台进程。框架结构如下图所示。&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20201014200000.png"
loading="lazy"
alt="20201014200000"
>&lt;/p>
&lt;ol>
&lt;li>Client
每一个 Job 在 Ciat 端将运行 MapRecdce 程序所需要的所有 Jar 文件和类的集合，打包成一个 Jar 文件存储在 HDFS 中，并把文件路径提交到 JobTracker。&lt;/li>
&lt;li>JobTracker
JobTracker 主要负责资源的监控和作业调度，一个 Hadoop 集群只有一个 JobTracker，并不参与具体的计算任务。根据提交的 Job，JobTackor 会创建一系列 Task（即 MapTask、ReduceTask），分发到每个 TaskTracker 服务中去执行。常用的作业调度算法主要包括 FIFO(First In First Out) 调度器（默认）、公平调度器、容量调度器等。&lt;/li>
&lt;li>TaskTracker
TaskTracker 主要负责汇报心跳和执行 JobTracker 分发的任务。TaskTracker 会周期性地通过 HeartBeat 将本节点上资源的使用情况和任务的运行进度汇报给 JobTracker，JobTracker 会根据心跳信息和当前作业运行情况为 TaskTracker 下达任务，主要包括启动任务、提交任务、杀死任务和重新初始化命令等。&lt;/li>
&lt;li>Task
Task 分为 MapTask 和 ReduceTask 两种，均由 TaskTracker 启动，执行 JobTracker 分发的任务。MapTask 解析每条数据记录，传递给用户编写的 map 函数并执行，最后将输出结果写入 HDFS；ReduceTask 从 MapTask 的执行结果中，对数据进行排序，将数据按分组传递给用户编写的 reduce 函数执行。&lt;/li>
&lt;/ol>
&lt;p>TaskTracker 分布在 Map-Reduce 集群每个节点上，主要是监视所在机器的资源情况和当前机器的 tasks 运行状况。TaskTracker 通过 HeartBeat 发送给 JobTracker，JobTracker 会根据这些信息给新提交的 job 分配计算节点。经典 MapReduce 框架 MR V1 模型简单直观，但是不能满足大规模集群任务调度的需要。主要表现为以下四点:&lt;/p>
&lt;ol>
&lt;li>JobTracker 是 MapReduce 的集中处理点，存在单点故障问题；&lt;/li>
&lt;li>当 MapRcduce job 非常多的时候，会造成很大的内存开销，就增加了 JobTracker 失败的风险，业界普遍认为该调度模型支持的上限为 4000 个节点;&lt;/li>
&lt;li>在 TaskTracker 端，以 Map/Reduce Task 的数目作为资源的表示过于简单，没有考虑到 CPU/内存的占用情况，如果两个大内存消耗的 Task 被调度到一起， 就很容易出现内存消耗殆尽的问题;&lt;/li>
&lt;li>TaskTracker 把资源强制划分为 Map Task Slot 和 Reduce Task Slot，如果当系统中只有 Map Task 或者只有 Reduce Task 时，会造成资源的浪费，导致集群资源利用不足。&lt;/li>
&lt;/ol>
&lt;h1 id="yarn-框架原理及运行机制">YARN 框架原理及运行机制&lt;/h1>
&lt;p>为了从根本上解决经典 MapReduce 框架的性能瓶颈，Hadoop 的 MapReduce 框架完全重构，叫做 YARN 或者 MR V2。&lt;/p>
&lt;p>YARN 的基本思想就是将经典调度框架中 JobTracker 的资源管理和任务调度/监控功能分离成两个单独的组件，即一个全局的资源管理器 ResoureManager 和每个应用程序特有的 ApplicationMaster。ResoureManager 负责整个系统资源的管理和分配，而 ApplicationMaster 则负责单个应用程序的资源管理。&lt;/p>
&lt;p>YARN 调度框架包括 ResourceManager、ApplicationMaster、NodeMananger 及 Container 等组件概念。&lt;/p>
&lt;p>ResourceManager 是基于应用程序对资源的需求进行调度的。每一个应用程序需要不同类型的资源，因此就需要不同的容器。这些资源包括内存、CPU、磁盘、网络等。
ApplicationMaster 负责向调度器申请、释放资源，清求 NodeManager 运行任务、跟踪应用程序的状态和监控它们的进程。&lt;/p>
&lt;p>NodeManager 是 YARN 中单个节点的代理，负责与应用程序的 ApplicationMaster 和集群管理者 ResourceManager 交互；从 ApplicationMaster 上接收有关 Container 的命令并执行（例如，启动、停止 Container）；向 ResourceManager 汇报各个 Container 执行状态和节点健康状况，并读取有关 Container 的命令；执行应用程序的容器、监控应用程序的资源使用情况并且向 ResourceManager 调度器汇报。&lt;/p>
&lt;p>Container 是 YARN 中资源的抽象，它封装了节点上一定量的资源（CPU 和内存等）。一个应用程序所需的 Container 分为两类：一类是运行 ApplicationMaster 的 Container，是由 ResourceManager（向内部的资源调度器）申请和启动的，用户提交应用程序时，可指定唯一的 ApplicationMaster 所需的资源；另一类是运行各类任务的 Container，是由 ApplicationMaster 向 ResourceManager 申请的，并由 ApplicationMaster 与 NodeManager 通信后启动。&lt;/p>
&lt;p>用户向 YARN 提交一个应用程序后，YARN 将分为两个阶段运行该应用程序：第一个阶段是启动 ApplicationMaster；第二个阶段是由 ApplicationMaster 创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行成功。&lt;/p>
&lt;p>YARN 任务调度流程如下图所示。&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20201014205100.jpg"
loading="lazy"
alt="20201014205100"
>&lt;/p>
&lt;ol>
&lt;li>用户向 YARN 提交应用程序；&lt;/li>
&lt;li>ResourceManager 为该应用程序在某个 NodeManagr 分配一个 Container，并要求 NodeManager 启动应用程序的 ApplicationMaster；&lt;/li>
&lt;li>ApplicationMaster 启动后立即向 ResourceManager 注册，此时用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后它将为各个任务申请分布在某些 NodeManager 上的容器资源，并监控它的运行状态（步骤 4~7），直到运行结束；&lt;/li>
&lt;li>ApplicationMaster 采用轮询的方式向 ResourceManager 申请和领取资源；&lt;/li>
&lt;li>ApplicationMaster 申请到资源后，即与资源容器所在的 NodeManager 通信，要求其在容器内启动任务;&lt;/li>
&lt;li>NodeManager 为任务初始化运行环境（包括环境变量、jar 包、二进制程序等)，启动任务；&lt;/li>
&lt;li>运行各个任务的容器通过向 ApplicationMaster 汇报自己的状态和进度，使 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。用户可以向 ApplicationMaster 查询应用程序的当前运行状态；&lt;/li>
&lt;li>应用程序运行完成后，ApplicationMaster 向 ResourceManager 注销并关闭。&lt;/li>
&lt;/ol>
&lt;p>YARN 框架和经典的 MRV1 调度框架相比，主要有以下优化：&lt;/p>
&lt;ol>
&lt;li>ApplicationMaster 使得检测每一个 Job 子任务状态的程序分布式化，减少了 JobTracker 资源消耗；&lt;/li>
&lt;li>在 YARN 中，用户可以对不同的编程模型写自己的 ApplicationMaster, 可以让更多类型的编程模型运行在 Hadoop 集群上，如 Spark 基于内存的计算模型；&lt;/li>
&lt;li>Container 提供 Java 虚拟机内存的隔离，优化了经典调度框架中 Map Slot 和 Reduce Slot 分开造成集群资源闲置的不足。&lt;/li>
&lt;/ol>
&lt;h1 id="youtube-数据集统计分析">Youtube 数据集统计分析&lt;/h1>
&lt;p>本例的数据来自于 Youtube 的数据集，完整的数据集以及源代码下载地址请点击以下链接
&lt;a class="link" href="https://github.com/jinggqu/BigDataTechnologyFoundation-SourceCodeAndDataSet/blob/main/ch04" target="_blank" rel="noopener"
>https://github.com/jinggqu/BigDataTechnologyFoundation-SourceCodeAndDataSet/blob/main/ch04&lt;/a>&lt;/p>
&lt;p>该数据集各字段的具体含义如表所示：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>字段名&lt;/th>
&lt;th>解释及数据类型&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>video ID&lt;/td>
&lt;td>视频 ID：每个视频存在唯一的 11 位字符串&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uploader&lt;/td>
&lt;td>上传者用户名：字符串类型&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>age&lt;/td>
&lt;td>视频上传日期与 2007 年 2 月 15 日（YouTube 创立日）的间隔天数：整数值&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>category&lt;/td>
&lt;td>视频类别：字符串类型&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>length&lt;/td>
&lt;td>视频长度：整数值&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>views&lt;/td>
&lt;td>浏览量：整数值&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>rate&lt;/td>
&lt;td>视频评分：浮点值&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ratings&lt;/td>
&lt;td>评分次数：整数值&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>comments&lt;/td>
&lt;td>评论数：整数值&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>related IDs&lt;/td>
&lt;td>相关视频 ID，每个相关视频的 ID 均为单独的一列：字符串类型&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="视频类型统计">视频类型统计&lt;/h2>
&lt;p>场景：从已经上传的视频中，统计每一个视频类型下的视频数量。下表所示为数据集数据格式示例。category 列代表了视频类型，因而 map 函数只需逐行读取，返回视频类型为键和数字 1 为值的键值对，再传给 reduce 函数处理即可。map 函数的输入键依然为文本文件中行的偏移量，值为行内容。reduce 函数输出键值对为视频类型和该视频类型中的视频数量。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>video ID&lt;/th>
&lt;th>uploader&lt;/th>
&lt;th>age&lt;/th>
&lt;th>category&lt;/th>
&lt;th>length&lt;/th>
&lt;th>views&lt;/th>
&lt;th>rate&lt;/th>
&lt;th>ratings&lt;/th>
&lt;th>comments&lt;/th>
&lt;th>Related IDs&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>PRGUU_ggO3k&lt;/td>
&lt;td>tom&lt;/td>
&lt;td>704&lt;/td>
&lt;td>Entertainment&lt;/td>
&lt;td>262&lt;/td>
&lt;td>11235&lt;/td>
&lt;td>3.86&lt;/td>
&lt;td>247&lt;/td>
&lt;td>280&lt;/td>
&lt;td>tpAL3iOurl4&amp;hellip;ifn1njiY4s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RX24KLBhwMI&lt;/td>
&lt;td>jsack&lt;/td>
&lt;td>687&lt;/td>
&lt;td>Blogs&lt;/td>
&lt;td>512&lt;/td>
&lt;td>24149&lt;/td>
&lt;td>4.22&lt;/td>
&lt;td>315&lt;/td>
&lt;td>474&lt;/td>
&lt;td>PkGUU_ggO3k&amp;hellip;tpAl3iOurl4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="mapper-类代码实现">Mapper 类代码实现&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">class&lt;/span> &lt;span class="nc">Map&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">extends&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Mapper&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">LongWritable&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Text&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Text&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IntWritable&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">private&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">final&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IntWritable&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ONE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IntWritable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">private&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">final&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Text&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tx&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Text&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">LongWritable&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Text&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Context&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">context&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IOException&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">InterruptedException&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">line&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">toString&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">[]&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">str&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">line&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;\t&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">if&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">str&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">length&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">3&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">this&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">tx&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">str&lt;/span>&lt;span class="o">[&lt;/span>&lt;span class="n">3&lt;/span>&lt;span class="o">]&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">context&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">this&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">tx&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ONE&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第 2 行构造 IntWritable 可持久化对象并赋值为 1；第 8~10 行过滤字段，将一条记录中的分类 category 作为 map 函数的 value 输出。&lt;/p>
&lt;h3 id="reduce-类代码实现">Reduce 类代码实现&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">class&lt;/span> &lt;span class="nc">Reduce&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">extends&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Reducer&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Text&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IntWritable&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Text&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IntWritable&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">reduce&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Text&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Iterable&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">IntWritable&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">values&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Context&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">context&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IOException&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">InterruptedException&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">for&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">IntWritable&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">values&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">get&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">context&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IntWritable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>reduce 函数接收 Map 阶段传来的键值对，第 3~6 行遍历每一组记录，累加同一视频类型下的视频数量，第 7 行通过 context 输出计算结果。&lt;/p>
&lt;h3 id="运行">运行&lt;/h3>
&lt;ol>
&lt;li>通过 IDEA-Build-Build Artifacts 功能将代码打包为 jar 文件，命名为 CategoryCount.jar&lt;/li>
&lt;li>登录 Hadoop 集群，将数据集文件 YoutubeDataSets.txt 传到 HDFS 下 /tmp 目录下&lt;/li>
&lt;li>执行如下命令，开始运行程序&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">hadoop jar CategoryCount.jar CategoryCount /tmp/YoutubeDataSets.txt /tmp/output
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="4">
&lt;li>执行如下命令，查看各类别视频数量&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">hadoop fs -cat /tmp/output/part-r-00000
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以得到如下输出&lt;/p>
&lt;pre>&lt;code>UNA 32
Autos &amp;amp; Vehicles 77
Comedy 420
Education 65
Entertainment 911
Film &amp;amp; Animation 261
Howto &amp;amp; Style 138
Music 870
News &amp;amp; Politics 343
Nonprofits &amp;amp; Activism 43
People &amp;amp; Blogs 399
Pets &amp;amp; Animals 95
Science &amp;amp; Technology 80
Sports 253
Travel &amp;amp; Events 113
&lt;/code>&lt;/pre></description></item><item><title>HDFS 文件管理</title><link>https://jinggqu.github.io/posts/hdfs-file-system/</link><pubDate>Mon, 12 Oct 2020 15:20:11 +0800</pubDate><guid>https://jinggqu.github.io/posts/hdfs-file-system/</guid><description>&lt;blockquote>
&lt;p>本文所有代码均可在 &lt;a class="link" href="https://github.com/jinggqu/HDFSOperations" target="_blank" rel="noopener"
>https://github.com/jinggqu/HDFSOperations&lt;/a> 查看。&lt;/p>
&lt;/blockquote>
&lt;h1 id="通过命令行访问-hdfs">通过命令行访问 HDFS&lt;/h1>
&lt;p>命令行是最简单、最直接操作文件的方式。这里介绍通过诸如读取文件、新建目录、移动文件、删除数据、列出目录等命令来进一步认识 HDFS。也可以输入 hadoop fs -help 命令获取每个命令的详细帮助。若熟悉 Linux 命令，Hadoop 命令看起来非常直观且易于使用。&lt;/p>
&lt;h2 id="对文件和目录的操作">对文件和目录的操作&lt;/h2>
&lt;p>通过命令行对 HDFS 文件和目录的操作主要包括：创建、浏览、删除文件和目录，以及从本地文件系统与 HDFS 文件系统互相拷贝等。常用命令格式如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">hadoop fs -ls &amp;lt;path&amp;gt; &lt;span class="c1"># 列出 path 目录下的所有内容（文件和目录）&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -lsr &amp;lt;path&amp;gt; &lt;span class="c1"># 递归列出 path 下的所有内容（文件或目录）&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -df &amp;lt;path&amp;gt; &lt;span class="c1"># 查看目录的使用情况&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -du &amp;lt;path&amp;gt; &lt;span class="c1"># 显示目录中所有文件及目录大小&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -touchz &amp;lt;path&amp;gt; &lt;span class="c1"># 创建一个路径为为 path 的 0 字节的 HDFS 空文件&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -mkdir &amp;lt;path&amp;gt; &lt;span class="c1"># 查看目录的使用情况&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -rm &lt;span class="o">[&lt;/span>-skipTrash&lt;span class="o">]&lt;/span> &amp;lt;path&amp;gt; &lt;span class="c1"># 将 HDFS 上路径为 &amp;lt;path&amp;gt; 的文件移动到回收站，加上 -skipTrash，则直接删除&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -rmr &lt;span class="o">[&lt;/span>-skipTrash&lt;span class="o">]&lt;/span> &amp;lt;path&amp;gt; &lt;span class="c1"># 将 HDFS 上路径为 &amp;lt;path&amp;gt; 的目录以及目录下的文件移动到回收站。如果加上 -skipTrash，则直接删除&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -moveFromLocal &amp;lt;localsrc&amp;gt;...&amp;lt;dst&amp;gt; &lt;span class="c1"># 将 &amp;lt;localsrc&amp;gt; 本地文件移动到 HDFS 的 &amp;lt;dst&amp;gt; 目录下路径下&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -moveToLocal &lt;span class="o">[&lt;/span>-crc&lt;span class="o">]&lt;/span> &amp;lt;src&amp;gt; &amp;lt;localdst&amp;gt; &lt;span class="c1"># 将 HDFS 上路径为 &amp;lt;src&amp;gt; 的文件移动到本地 &amp;lt;localdst&amp;gt; 路径下&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -put &amp;lt;localsrc&amp;gt;...&amp;lt;dst&amp;gt; &lt;span class="c1"># 从本地文件系统中复制单个或者多个源路径到目标文件系统&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hadoop fs -cat &amp;lt;src&amp;gt; &lt;span class="c1"># 浏览 HDFS 路径为 &amp;lt;src&amp;gt; 的文件的内容&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="修改权限或用户组">修改权限或用户组&lt;/h2>
&lt;p>HDFS 提供了一些命令可以用来修改文件的权限、所属用户以及所属组别，具体格式如下:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;code>hadoop fs -chmod [-R] &amp;lt;MODE [,MODE]...|OCTALMODE&amp;gt; PATH...&lt;/code>&lt;br>
改变 HDFS 上路径为 PATH 的文件的权限，R 选项表示递归执行该操作。&lt;br>
例如: &lt;code>hadoop fs -chmod -R +r /user/test&lt;/code>，表示将 /user/test 目录下的所有文件赋予读的权限&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hadoop fs -chown [-R][OWNER][:[GROUP]]PATH...&lt;/code>&lt;br>
改变 HDFS 上路径为 PATH 的文件的所属用户，-R 选项表示递归执行该操作。&lt;br>
例如: &lt;code>hadoop fs -chown -R hadoop:hadoop /user/test&lt;/code>，表示将 /user/test 目录下所有文件的所属用户和所属组别改为 hadoop&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hadoop fs -chgrp ［-R] GROUP PATH...&lt;/code>&lt;br>
改变 HDFS 上路径为 PATH 的文件的所属组别，-R 选项表示递归执行该操作。&lt;br>
例如: &lt;code>hadoop fs -chown -R hadoop /user/test&lt;/code> 表示将 /user/test 目录下所有文件的所属组别改为 hadoop&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="其他命令">其他命令&lt;/h2>
&lt;p>HDFS 除了提供上述两类操作之外，还提供许多实用性较强的操作，如显示指定路径上的内容，上传本地文件到 HDFS 指定文件夹，以及从 HDFS 上下载文件到本地等命令。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;code>hadoop fs -tail [-f] &amp;lt;file&amp;gt;&lt;/code>&lt;br>
显示 HDFS 上路径为 &amp;lt;file&amp;gt; 的文件的最后 1KB 的字节，-f 选项会使显示的内容随着文件内容更新而更新。&lt;br>
例如: &lt;code>hadoop fs -tail -f /user/test.txt&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hadoop fs -stat [format] &amp;lt;path&amp;gt;&lt;/code>&lt;br>
显示 HDFS 上路径为 &amp;lt;path&amp;gt; 的文件或目录的统计信息。格式为：%b 文件大小，%n 文件名，%r 复制因子，%y、%Y 修改日期。&lt;br>
例如：&lt;code>hadoop fs -stat %b %n %o %r /user/test&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hadoop fs -put &amp;lt;localsrc&amp;gt;...&amp;lt;dt&amp;gt;&lt;/code>&lt;br>
将 &amp;lt;localsrc&amp;gt; 本地文件上传到 HDFS 的 &amp;lt;dst&amp;gt; 目录下。&lt;br>
例如: &lt;code>hadoop fs -put /home/hadoop/test.txt /user/hadoop&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hadoop fs -count [-q] &amp;lt;path&amp;gt;&lt;/code>&lt;br>
显示 &amp;lt;path&amp;gt; 下的目录数及文件数，输出格式为”目录数 文件数 大小 文件名“，加上 -q 可以查看文件索引的情况。&lt;br>
例如: &lt;code>hadoop fs -count /&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hadoop fs -get [-ignoreCrc] [-crc] &amp;lt;src&amp;gt; &amp;lt;localdst&amp;gt;&lt;/code>&lt;br>
将 HDFS 上 &amp;lt;src&amp;gt; 的文件下载到本地的 &amp;lt;localdst&amp;gt; 目录，可用 -ignorecrc 选项复制 CRC 校验失败的文件，使用 -crc 选项复制文件以及 CRC 信息。&lt;br>
例如: &lt;code>hadoop fs -get /user/hadoop/a.txt /home/hadoop&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hadoop fs -getmerge &amp;lt;src&amp;gt; &amp;lt;localdst&amp;gt; [addnl]&lt;/code>&lt;br>
将 HDFS 上 &amp;lt;src&amp;gt; 目录下的所有文件按文件名排序并合并成一个文件输出到本地的 &amp;lt;localdst&amp;gt; 目录，addnl 是可选的，用于指定在每个文件结尾添加一个换行符。&lt;br>
例如: &lt;code>hadoop fs -getmerge /user/test /home/hadoop/o&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hadoop fs -test -[ezd] &amp;lt;path&amp;gt;&lt;/code>&lt;br>
检查 HDFS 上路径为 &amp;lt;path&amp;gt; 的文件。-e 检查文件是否存在，如果存在则返回 0。-z 检查文件是否为 0 字节，如果是则返回 0。-d 检查路径是否是目录，如果是则返回 1，否则返回 0。&lt;br>
例如：&lt;code>hadoop fs -test -e /user/test.txt&lt;/code>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h1 id="通过-java-api-访问-hdfs">通过 Java API 访问 HDFS&lt;/h1>
&lt;h2 id="使用-hadoop-url-读取数据">使用 Hadoop URL 读取数据&lt;/h2>
&lt;p>要从 Hadoop 文件系统读取数据，最简单的方法是使用 java.net.URL 对象打开数据流，从中读取数据。&lt;/p>
&lt;p>让 Java 程序能够识别 Hadoop 的 HDFS URL 方案还需要一些额外的工作，这里采用的方法是通过 org.apache.hdfs.FsUrlStreamHandlerFactor 实例调用 java.net.URL 对象的 setURLStreamHandlerFactory 实例方法。每个 Java 虚拟机只能调用一次这个方法，因此通常在静态方法中调用。下述范例展示的程序以标准输出方式显示 Hadoop 文件系统中的文件，类似于 UNIX 中的 cat 命令。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.hadoop.fs.FsUrlStreamHandlerFactory&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">org.apache.hadoop.io.IOUtils&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">java.io.InputStream&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kn">import&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nn">java.net.URL&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">class&lt;/span> &lt;span class="nc">URLCat&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">URL&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">setURLStreamHandlerFactory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FsUrlStreamHandlerFactory&lt;/span>&lt;span class="p">());&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">main&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">[]&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Exception&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">InputStream&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">inputStream&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">null&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">try&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">inputStream&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">URL&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">args&lt;/span>&lt;span class="o">[&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">]&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="na">openStream&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">IOUtils&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">copyBytes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputStream&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">4096&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">finally&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">IOUtils&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">closeStream&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputStream&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>编译代码，导出为 URLCat.jar 文件，并在 /user/hadoop/ 中准备一个测试文件 test，然后执行命令：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">hadoop jar hdfsclient.jar URLCat hdfs://master:9000/user/hadoop/test
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>执行完成后可以在屏幕上看到 /user/hadoop/test 文件中的内容。该程序是从 HDFS 读取文件的最简单的方式，即用 java.net.URL 对象打开数据流。其中，第 8~10 行静态代码块的作用是设置 URL 类能够识别 Hadoop 的 HDFS URL。第 16 行 IOUtils 是 Hadoop 中定义的类，调用其静态方法 copyBytes 实现从 HDFS 文件系统拷贝文件到标准输出流。4096 表示用来拷贝的缓冲区大小，false 表示拷贝完成后不关闭拷贝源。&lt;/p>
&lt;h2 id="通过-filesystem-api-读取数据">通过 FileSystem API 读取数据&lt;/h2>
&lt;p>在实际开发中，访问 HDFS 最常用的类是 FileSystem 类。Hadoop 文件系统中通过 Hadoop Path 对象来定位文件。可以将路径视为一个 Hadoop 文件系统 URI，如 hdfs:localhost/user/hadoop/test。FileSystem 是一个通用的文件系统 API，获取 FileSystem 实例有下面几个静态方法:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">conf&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IOException&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">URI&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">conf&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IOException&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">static&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">URI&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">conf&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">user&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">throw&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">IOException&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>下面分别给出几个常用操作的代码示例。&lt;/p>
&lt;h3 id="读取文件">读取文件&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@Test&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">readFile&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Exception&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34;hdfs://master:9000/user/hadoop/test&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">URI&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">InputStream&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">in&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">null&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">try&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">in&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Path&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">IOUtils&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">copyBytes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">in&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">4096&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">finally&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">IOUtils&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">closeStream&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">in&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>上述代码直接使用 FileSystem 以标准输出格式显示 Hadoop 文件系统中的文件。&lt;/p>
&lt;p>第 4 行产生一个 Confguation 类的实例，代表了 Hadoop 平台的配置信息，并在第 5 行作为引用传递到 FileSystem 的静态方法 get 中，产生 FileSystem 对象。&lt;/p>
&lt;p>第 9 行与上例类似，调用 Hadoop 中 IOUtils，并在 finally 字中关闭数据流，同时也可以在输入流和输出流之间复制数据。copyBytes 方的最后两个参数，第一个设置用于复制的缓冲区大小，第二个设置复制结束后是否关闭数据流。&lt;/p>
&lt;h3 id="写入文件">写入文件&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@Test&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">writeFile&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Exception&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">source&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34;C:\\Users\\Desktop\\test&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">destination&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34;hdfs://master:9000/user/hadoop/test2&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">BufferedInputStream&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">inputStream&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">BufferedInputStream&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FileInputStream&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">source&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">URI&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">destination&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">OutputStream&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">outputStream&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Path&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">destination&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">IOUtils&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">copyBytes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputStream&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">outputStream&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">4096&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="创建目录">创建目录&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@Test&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">createFolder&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Exception&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34;hdfs://master:9000/user/test&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">URI&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">Path&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">path&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Path&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">mkdirs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">path&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">close&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="删除文件或目录">删除文件或目录&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@Test&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">deleteFile&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Exception&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34;hdfs://master:9000/user/hadoop/test2&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">URI&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">Path&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">path&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Path&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;hdfs://master:9000/user/hadoop&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="kt">boolean&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">isDeleted&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">delete&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">path&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">isDeleted&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">close&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用 FileSystem 的 delete() 方法可以永久性删除文件或目录。如果要递归删除文件夹，则需要将其第二个参数设为 true。&lt;/p>
&lt;h3 id="列出文件或目录">列出文件或目录&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@Test&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="kd">public&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kt">void&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nf">listFiles&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kd">throws&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Exception&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34;hdfs://master:9000/user&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Configuration&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">FileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">URI&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">configuration&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">Path&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">path&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">new&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Path&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">FileStatus&lt;/span>&lt;span class="o">[]&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">status&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">listStatus&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">path&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">for&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">FileStatus&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">fileStatus&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">status&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">System&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">fileStatus&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">getPath&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="na">toString&lt;/span>&lt;span class="p">());&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">fileSystem&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="na">close&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>文件系统的重要特性是提供浏览和检索其目录结构下所存文件与目录相关信息的功能。 FileStatus 类封装了文件系统中文件和目录的元数据，例如文件长度、块大小、副本、修改时间、所有者以及权限信息等。编译运行上述代码后控制台将会打印出 /user 目录下的名称或者文件名。&lt;/p>
&lt;h1 id="小结">小结&lt;/h1>
&lt;h2 id="hdfs-组成部分">HDFS 组成部分&lt;/h2>
&lt;ul>
&lt;li>HDFS 是一个分布式文件存储系统&lt;/li>
&lt;li>Client 提交读写请求（拆分 blocksize）&lt;/li>
&lt;li>NameNode 全局把控（存储数据位置）&lt;/li>
&lt;li>DataNode 存储数据（将数据存储进去，且以 Pipeline 的方式把数据写完）&lt;/li>
&lt;/ul>
&lt;h2 id="hdfs-数据交互">HDFS 数据交互&lt;/h2>
&lt;h3 id="写入数据">写入数据&lt;/h3>
&lt;ol>
&lt;li>使用 HDFS 提供的客户端 Client，向远程的 NameNode 发起 RPC 请求&lt;/li>
&lt;li>NameNode 会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会为文件创建一个记录，否则会让客户端抛出异常&lt;/li>
&lt;li>当客户端开始写入文件的时候，客户端会将文件切分成多个 packets，并在内部以数据队列 data queue（数据队列） 的形式管理这些 packets，并向 NameNode 申请 blocks，获取用来存储 replicas 的合适的 DataNode 列表，列表的大小根据 NameNode 中 replication（副本份数）的设定而定&lt;/li>
&lt;li>开始以 pipeline（管道）的形式将 packet 写入所有的 replicas 中。客户端把 packet 以流的方式写入第一个 DataNode，该 DataNode 把该 packet 存储之后，再将其传递给在此 pipeline 中的下一个 DataNode，直到最后一个 DataNode，这种写数据的方式呈流水线的形式&lt;/li>
&lt;li>最后一个 DataNode 成功存储之后会返回一个 ack packet（确认队列），在 pipeline 里传递至客户端，在客户端的开发库内部维护着 &amp;ldquo;ack queue&amp;rdquo;，成功收到 DataNode 返回的 ack packet 后会从 &amp;ldquo;data queue&amp;rdquo; 移除相应的 packet&lt;/li>
&lt;li>如果传输过程中，有某个 DataNode 出现了故障，那么当前的 pipeline 会被关闭，出现故障的 DataNode 会从当前的 pipeline 中移除，剩余的 block 会继续剩下的 DataNode 中继续以 pipeline 的形式传输，同时 NameNode 会分配一个新的 DataNode，保持 replicas 设定的数量。&lt;/li>
&lt;li>客户端完成数据的写入后，会对数据流调用 close() 方法，关闭数据流&lt;/li>
&lt;li>只要写入了 dfs.replication.min（最小写入成功的副本数）的复本数（默认为 1），写操作就会成功，并且这个块可以在集群中异步复制，直到达到其目标复本数（dfs.replication 的默认值为 3），因为 NameNode 已经知道文件由哪些块组成，所以它在返回成功前只需要等待数据块进行最小量的复制&lt;/li>
&lt;/ol>
&lt;h3 id="读取数据">读取数据&lt;/h3>
&lt;ol>
&lt;li>客户端调用 FileSystem 实例的 open 方法，获得这个文件对应的输入流 InputStream&lt;/li>
&lt;li>通过 RPC 远程调用 NameNode，获得 NameNode 中此文件对应的数据块保存位置，包括这个文件的副本的保存位置（主要是各 DataNode 的地址）&lt;/li>
&lt;li>获得输入流之后，客户端调用 read 方法读取数据。选择最近的 DataNode 建立连接并读取数据&lt;/li>
&lt;li>如果客户端和其中一个 DataNode 位于同一机器（比如 MapReduce 过程中的 mapper 和 reducer)，那么就会直接从本地读取数据&lt;/li>
&lt;li>到达数据块末端，关闭与这个 DataNode 的连接，然后重新查找下一个数据块&lt;/li>
&lt;li>不断执行第 2~5 步直到数据全部读完&lt;/li>
&lt;li>客户端调用 close，关闭输入流 DFS InputStream&lt;/li>
&lt;/ol>
&lt;h1 id="hdfs-漫画">HDFS 漫画&lt;/h1>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20201023190000.png"
loading="lazy"
alt="20201023190000"
>&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20201023190100.png"
loading="lazy"
alt="20201023190100"
>&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20201023190200.png"
loading="lazy"
alt="20201023190200"
>&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20201023190300.png"
loading="lazy"
alt="20201023190300"
>&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20201023190400.png"
loading="lazy"
alt="20201023190400"
>&lt;/p>
&lt;blockquote>
&lt;p>以上漫画版权均归原图作者所有&lt;/p>
&lt;/blockquote>
&lt;h1 id="参考文章">参考文章&lt;/h1>
&lt;p>&lt;a class="link" href="https://www.cnblogs.com/qingyunzong/p/8548806.html" target="_blank" rel="noopener"
>https://www.cnblogs.com/qingyunzong/p/8548806.html&lt;/a>&lt;/p></description></item><item><title>初识 Hadoop</title><link>https://jinggqu.github.io/posts/getting-to-know-hadoop/</link><pubDate>Thu, 24 Sep 2020 09:20:11 +0800</pubDate><guid>https://jinggqu.github.io/posts/getting-to-know-hadoop/</guid><description>&lt;h1 id="前言">前言&lt;/h1>
&lt;p>本系列文章是基于《大数据技术基础》与 &lt;a class="link" href="https://coding.imooc.com/class/128.html" target="_blank" rel="noopener"
>10 小时入门大数据&lt;/a> 课程，如果有兴趣可以先阅读该书并观看视频教程。本系列文章中所用到的软件版本及其下载地址如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>名称&lt;/th>
&lt;th>版本&lt;/th>
&lt;th>下载地址&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>CentOS&lt;/td>
&lt;td>8.2.2004&lt;/td>
&lt;td>&lt;a class="link" href="https://mirrors.tuna.tsinghua.edu.cn/centos/8.2.2004/isos/x86_64/CentOS-8.2.2004-x86_64-minimal.iso" target="_blank" rel="noopener"
>https://mirrors.tuna.tsinghua.edu.cn/centos/8.2.2004/isos/x86_64/CentOS-8.2.2004-x86_64-minimal.iso&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>JDK&lt;/td>
&lt;td>14.0.2&lt;/td>
&lt;td>&lt;a class="link" href="https://www.oracle.com/java/technologies/javase/jdk14-archive-downloads.html" target="_blank" rel="noopener"
>https://www.oracle.com/java/technologies/javase/jdk14-archive-downloads.html&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hadoop&lt;/td>
&lt;td>2.10.1&lt;/td>
&lt;td>&lt;a class="link" href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.10.1/hadoop-2.10.1-src.tar.gz" target="_blank" rel="noopener"
>https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.10.1/hadoop-2.10.1-src.tar.gz&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="环境准备">环境准备&lt;/h1>
&lt;h2 id="配置网络">配置网络&lt;/h2>
&lt;p>此篇文章所使用的 CentOS 环境均是使用 VMware 15 虚拟的，具体安装教程请查看 &lt;a class="link" href="https://www.cnblogs.com/bobo-pcb/p/11708459.html" target="_blank" rel="noopener"
>使用 VMware 15 安装虚拟机和使用 CentOS 8&lt;/a>，此处不再赘述。安装好一个节点之后，我们可以采用“虚拟机克隆”的方式，直接完成另外两个节点系统的安装。&lt;/p>
&lt;p>虚拟机的网络配置采用 DHCP 自动分配模式，每台机器的 IP 地址可以通过命令 &lt;code>ip address&lt;/code> 或 &lt;code>ifconfig&lt;/code> 查看，其中 &lt;code>ifconfig&lt;/code> 输出如下，第一组配置中 &lt;code>ens33&lt;/code> 即为本机网络配置，&lt;code>inet&lt;/code> 项对应的即为本机 ip（192.168.61.128）。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">&lt;span class="line">&lt;span class="cl">ens33: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt; mtu 1500
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> inet 192.168.61.128 netmask 255.255.255.0 broadcast 192.168.61.255
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> inet6 fe80::20c:29ff:fe65:9052 prefixlen 64 scopeid 0x20&amp;lt;link&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ether 00:0c:29:65:90:52 txqueuelen 1000 (Ethernet)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> RX packets 38037 bytes 6542757 (6.2 MiB)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> RX errors 0 dropped 0 overruns 0 frame 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> TX packets 30479 bytes 16809162 (16.0 MiB)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">lo: flags=73&amp;lt;UP,LOOPBACK,RUNNING&amp;gt; mtu 65536
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> inet 127.0.0.1 netmask 255.0.0.0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> inet6 ::1 prefixlen 128 scopeid 0x10&amp;lt;host&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> loop txqueuelen 1000 (Local Loopback)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> RX packets 23656 bytes 13542580 (12.9 MiB)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> RX errors 0 dropped 0 overruns 0 frame 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> TX packets 23656 bytes 13542580 (12.9 MiB)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">virbr0: flags=4099&amp;lt;UP,BROADCAST,MULTICAST&amp;gt; mtu 1500
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ether 52:54:00:d2:b3:31 txqueuelen 1000 (Ethernet)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> RX packets 0 bytes 0 (0.0 B)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> RX errors 0 dropped 0 overruns 0 frame 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> TX packets 0 bytes 0 (0.0 B)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>本篇文章中三台集群的 IP 分别如下，下文中不再赘述。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">主机名&lt;/th>
&lt;th style="text-align:center">IP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">master&lt;/td>
&lt;td style="text-align:center">192.168.61.128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">slave1&lt;/td>
&lt;td style="text-align:center">192.168.61.129&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">slave2&lt;/td>
&lt;td style="text-align:center">192.168.61.131&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="配置-host">配置 host&lt;/h2>
&lt;p>以上三台机器要搭建成为集群，就需要让它们互相认识。这个认识的过程是通过 &lt;code>/etc/hosts&lt;/code> 文件来实现的。这一步需要修改每一台机器的 hosts 文件，将以下内容分别粘贴到各个机器的 hosts 文件中。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">vim /etc/hosts
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">192.168.61.128 master
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">192.168.61.129 slave1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">192.168.61.131 slave2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="配置-jdk">配置 JDK&lt;/h2>
&lt;p>因为 Hadoop 的环境依赖于 Java JDK，所以需要确保虚拟机中已经正确安装了 JDK，除此之外我们还需要将 JDK 地址配置到环境变量中。在本例中，我的 JDK 安装位置是 &lt;code>/usr/java/jdk-14.0.2&lt;/code>。&lt;/p>
&lt;h3 id="修改-bash_profile">修改 bash_profile&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">vim ~/.bash_profile
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>添加以下内容到 .bash_profile 文件末尾：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="k">export&lt;/span> &lt;span class="n">JAVA_HOME&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">usr&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">java&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">jdk&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">14.0&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">export&lt;/span> &lt;span class="n">PATH&lt;/span>&lt;span class="o">=$&lt;/span>&lt;span class="n">JAVA_HOME&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">bin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">PATH&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>修改完成并保存后，还需要执行 &lt;code>source&lt;/code> 命令使环境变量立即生效。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">source&lt;/span> ~/.bash_profile
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>然后即可使用 &lt;code>java -version&lt;/code> 检查环境变量是否配置成功，执行结果如下所示。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">java version &amp;#34;14.0.2&amp;#34; 2020-07-14
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Java(TM) SE Runtime Environment (build 14.0.2+12-46)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Java HotSpot(TM) 64-Bit Server VM (build 14.0.2+12-46, mixed mode, sharing)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="配置-ssh-免密钥登录">配置 SSH 免密钥登录&lt;/h2>
&lt;p>在 Linux 集群间配置免密钥登录，是 Hadoop 集群运维的基础。以下操作在 master 节点进行，实现从 master 免密钥登录 slave1、slave2 节点。生成 ssh 密钥的命令如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ssh-keygen
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>生成过程中会有一些提示，一路回车即可。执行结果如下所示。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">root@master:/usr/local/software# ssh-keygen
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Generating public/private rsa key pair.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Enter file in which to save the key (/root/.ssh/id_rsa):
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/root/.ssh/id_rsa already exists.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Overwrite (y/n)? y
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Enter passphrase (empty for no passphrase):
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Enter same passphrase again:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Your identification has been saved in /root/.ssh/id_rsa.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Your public key has been saved in /root/.ssh/id_rsa.pub.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">The key fingerprint is:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">SHA256:DC7+sETaazn0f4OVgxozjdw2XM1Tb60cqoaQvDGXpg8 root@master
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">The key&amp;#39;s randomart image is:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">+---[RSA 3072]----+
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">| |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">| . |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">| . o . ..|
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">| . o . + . +|
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">| oo.*S+ . + + |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">| =..% @ + . o |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">| ..=oE# = o |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">| .+==.o = |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">| .o..ooo . |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">+----[SHA256]-----+
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>接下来需要将生成的公钥上传到 slave1 节点，命令如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ssh-copy-id root@slave1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>首次通过 master 终端将公钥传给 salve 终端，需要输入 slave 节点的登录密码。上述命令中我们是传输到 slave1 的 root 账户下，所以需要输入 root 用户的密码，传送完毕即可实现免密码登录。执行结果如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">root@master:/usr/local/software# ssh-copy-id root@slave1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &amp;#34;/root/.ssh/id_rsa.pub&amp;#34;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">root@slave1&amp;#39;s password:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Number of key(s) added: 1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Now try logging into the machine, with: &amp;#34;ssh &amp;#39;root@slave1&amp;#39;&amp;#34;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">and check to make sure that only the key(s) you wanted were added.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>slave2 节点命令同上，只需更改传送到的节点名称，执行结果如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">root@master:/usr/local/software# ssh-copy-id root@slave2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &amp;#34;/root/.ssh/id_rsa.pub&amp;#34;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">root@slave2&amp;#39;s password:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Number of key(s) added: 1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Now try logging into the machine, with: &amp;#34;ssh &amp;#39;root@slave2&amp;#39;&amp;#34;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">and check to make sure that only the key(s) you wanted were added.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>现在可以尝试登录子节点 slave1 和 slave2。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ssh root@slave1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>成功登录 salve1 节点的提示如下。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">root@master:/usr/local/software# ssh root@slave1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Web console: https://slave1:9090/ or https://192.168.61.129:9090/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Last login: Fri Sep 24 14:56:46 2020 from 192.168.61.1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="完善配置">完善配置&lt;/h1>
&lt;p>以下配置均在 master 节点上完成，配置完成后可直接复制到 slave 节点，以免重复劳动。&lt;/p>
&lt;h2 id="安装-hadoop">安装 Hadoop&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> /usr/local/software
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">wget http://mirror.cogentco.com/pub/apache/hadoop/common/hadoop-2.10.1/hadoop-2.10.1-src.tar.gz
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">tar -zxvf hadoop-2.10.1-src.tar.gz
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> hadoop-2.10.1-src
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">mv * ~/hadoop
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在正式使用 Hadoop 集群之前，我们还需要对其配置文件进行修改。本节中的配置内容请以 &lt;a class="link" href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener"
>官方文档&lt;/a> 为准。&lt;/p>
&lt;p>Hadoop 的配置文件均存放在 Hadoop 所在目录的 /etc/hadoop/ 文件夹下。&lt;/p>
&lt;h2 id="修改配置文件">修改配置文件&lt;/h2>
&lt;h3 id="编辑-core-sitexml">编辑 core-site.xml&lt;/h3>
&lt;p>文件 core-site.xml 用来配置 Hadoop 集群的通用属性，包括指定 NameNode 的地址、指定使用 Hadoop 时临时文件的存放路径、指定检查点备份日志的最长时间等。&lt;/p>
&lt;p>使用 vim 打开文件：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">vim ~/hadoop-2.10.1/etc/hadoop/core-site.xml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用以下内容替换 core-site.xml 中的内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&amp;lt;?xml-stylesheet type=&amp;#34;text/xsl&amp;#34; href=&amp;#34;configuration.xsl&amp;#34;?&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;configuration&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c">&amp;lt;!-- 指定 namenode 的地址 --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>fs.defaultFS&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>hdfs://master:9000&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c">&amp;lt;!-- 指定使用 Hadoop 时临时文件的存放路径 --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>hadoop.tmp.dir&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>/home/hadoop/temp&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/configuration&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第 6~9 行配置 fs.defaultFS 的属性为 hdfs://master:9000，master 是主机名；第 12~15 行指定 Hadoop 的临时文件夹为 /home/hadoop/temp，此文件夹用户可以自己指定。&lt;/p>
&lt;h3 id="编辑-hdfs-sitexml">编辑 hdfs-site.xml&lt;/h3>
&lt;p>文件 hdfs-site.xml 用来配置分布式文件系统 HDFS 的属性，包括指定 HDFS 保存数据的副本数量，指定 HDFS 中 NameNode、DataNode 的存储位置等。&lt;/p>
&lt;p>使用 vim 打开文件：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">vim ~/hadoop-2.10.1/etc/hadoop/hdfs-site.xml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用以下内容替换 hdfs-site.xml 中的内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&amp;lt;?xml-stylesheet type=&amp;#34;text/xsl&amp;#34; href=&amp;#34;configuration.xsl&amp;#34;?&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;configuration&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c">&amp;lt;!-- 指定 HDFS 保存数据的副本数量 --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>dfs.replication&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>1&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/configuration&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中，第 7~8 行，指定 HDFS 文件快的副本数为 1。数据块副本一般为 3 以上，本文章仅作示例，故指定为 1。&lt;/p>
&lt;h3 id="编辑-yarn-sitexml">编辑 yarn-site.xml&lt;/h3>
&lt;p>YARN 是 MapReduce 的调度框架。文件 yarn-site.xml 用配置 YARN 的属性，包括指定 NameNodeManager 获取数据的方式，指定 ResourceManager 的地址，配置 YARN 打印工作日志等。&lt;/p>
&lt;p>使用 vim 打开文件：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">vim ~/hadoop-2.10.1/etc/hadoop/yarn-site.xml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用以下内容替换 yarn-site.xml 中的内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&amp;lt;?xml version=&amp;#34;1.0&amp;#34;?&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;configuration&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c">&amp;lt;!-- 指定 NameNodeManager 获取数据的方式是 shuffle --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>yarn.nodemanager.aux-services&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>mapreduce_shuffle&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>yarn.nodemanager.env-whitelist&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c">&amp;lt;!-- 指定 YARN 中 ResourceManager 所在的主机名 --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>yarn.resourcemanager.hostname&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>master&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/configuration&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中，第 15~19 行配置了 ResourceManager 所在的主机名，如果不进行配置，将会导致 MapReduce 不能获得资源，任务不能执行。&lt;/p>
&lt;h3 id="编辑-mapred-sitexml">编辑 mapred-site.xml&lt;/h3>
&lt;p>文件 mapred-site.xml 主要是配置 MapReduce 的属性，主要是 Hadoop 系统提交的 Map/Reduce 程序运行在 YARN 上。&lt;/p>
&lt;p>首先复制一份 mapred-site.xml.template 文件为 mapred-site.xml，然后打开并进行修改。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">vim ~/hadoop-2.10.1/etc/hadoop/mapred-site.xml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用以下内容替换 mapred-site.xml 中的内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&amp;lt;?xml version=&amp;#34;1.0&amp;#34;?&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&amp;lt;?xml-stylesheet type=&amp;#34;text/xsl&amp;#34; href=&amp;#34;configuration.xsl&amp;#34;?&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;configuration&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>mapreduce.framework.name&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>yarn&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>mapreduce.application.classpath&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/configuration&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中，第 5~8 行为 MapReduce 指定任务调度框架为 YARN。&lt;/p>
&lt;h3 id="编辑-slaves">编辑 slaves&lt;/h3>
&lt;p>slaves 文件为 Hadoop 提供了子节点的主机名。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">vim ~/hadoop-2.10.1/etc/hadoop/slaves
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用以下内容替换 slaves 中的内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">slave1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">slave2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="复制文件到子节点">复制文件到子节点&lt;/h2>
&lt;p>使用下面的命令将 Hadoop 文件复制到其他节点，本文中为 slave1 和 slave2，命令如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> ~/hadoop
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">scp -r hadoop-2.10.1 root@slave1:~/hadoop/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">scp -r hadoop-2.10.1 root@slave2:~/hadoop/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="配置-hadoop-环境变量">配置 Hadoop 环境变量&lt;/h2>
&lt;p>注意，此操作需要同时在所有节点（master，slave1，slave2）都执行一次，操作命令如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">vim ~/.bash_profile
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>将以下内容追加到 .bash_profile 文件末尾：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#HADOOP&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">export&lt;/span> &lt;span class="n">HADOOP_HOME&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">hadoop&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">hadoop&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">2.10&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">export&lt;/span> &lt;span class="n">PATH&lt;/span>&lt;span class="o">=$&lt;/span>&lt;span class="n">HADOOP_HOME&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">bin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">HADOOP_HOME&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">sbin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">$&lt;/span>&lt;span class="n">PATH&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>然后执行下列命令使环境变量生效：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">source&lt;/span> ~/.bash_profile
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="创建临时文件存放目录">创建临时文件存放目录&lt;/h2>
&lt;p>我们在 core-site.xml 文件中指定了 Hadoop 临时文件存放路径，但是文件夹并没有创建，此操作需要同时在所有节点（master，slave1，slave2）都执行一次，操作命令如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">mkdir /home/hadoop/temp
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="启动集群">启动集群&lt;/h1>
&lt;h2 id="格式化文件系统">格式化文件系统&lt;/h2>
&lt;p>注意，格式化仅需要在第一次使用 Hadoop 集群时进行，后续使用时无需格式化，并且在使用过程中进行格式化，所有文件将会丢失。此操作需要在 master 节点上进行，执行如下命令：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">hdfs namenode -format
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="启动-hadoop-集群">启动 Hadoop 集群&lt;/h2>
&lt;p>Hadoop 启动或停止服务的脚本均存放在 sbin 目录中，所以切换到 /home/hadoop/hadoop-2.10.1/sbin 目录下，执行以下命令：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">start-all.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>需要注意的是，在启动过程中，Hadoop 会提示这样的启动方式已经过时，使用如下启动方式即可规避过时提示：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">start-dfs.sh
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">start-yarn.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="查看进程是否启动成功">查看进程是否启动成功&lt;/h2>
&lt;p>在 master 节点终端执行 jps 命令，在打印结果中会看到四个进程，分别是 NodeManager、SecondaryNameNode、ResourceManager、Jps。如果出现了这四个进程表示启动成功。结果如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">root@master:~/hadoop/hadoop-2.10.1/sbin# jps
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">17874 NameNode
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">18070 SecondaryNameNode
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">18281 ResourceManager
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">18554 Jps
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>此时在 slave1 和 slave2 的节点的终端执行 jps 命令，在输出结果中会看到三个进程，分别是 Jps、NodeManager、DataNode，如果出现了这三个进程表示子节点进程启动成功。结果如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">root@slave1:~# jps
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">15776 NodeManager
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">15639 DataNode
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">16106 Jps
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="查看-webui">查看 WebUI&lt;/h2>
&lt;h3 id="hadoop-页面">Hadoop 页面&lt;/h3>
&lt;p>如果要在宿主机上访问虚拟机 master 节点的 WebUI，需要先将虚拟机的防火墙关闭（此处仅仅是做示例，生产环境不建议这么做），然后访问虚拟机 master 节点 IP:50070 即可。&lt;/p>
&lt;p>防火墙相关命令如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 暂时关闭防火墙&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">systemctl stop firewalld
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 永久关闭防火墙&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">systemctl disable firewalld
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 启用防火墙&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">systemctl &lt;span class="nb">enable&lt;/span> firewalld
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>例如本例中 master 节点地址为 192.168.61.128，则访问 192.168.61.128:50070，页面如下图：&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20200926201409.png"
loading="lazy"
alt="20200926201409"
>&lt;/p>
&lt;h3 id="yarn-页面">YARN 页面&lt;/h3>
&lt;p>如上例，与 Hadoop 管理页面不同的是，YARN Web 页面地址端口是 8088，页面如下图：&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20200926203912.png"
loading="lazy"
alt="20200926203912"
>&lt;/p>
&lt;h1 id="运行实例">运行实例&lt;/h1>
&lt;p>在 Hadoop 自带的 examples 中有一种利用分布式系统计算圆周率的方法，采用的是拟蒙特卡罗（Quasi-Monte Carlo）算法来对 $ \pi $ 的值进行估算。下面通过运行该程序来检验 Hadoop 集群是否安装配置成功。&lt;/p>
&lt;p>在 master 节点终端中执行下面的命令：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">hadoop jar hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar pi &lt;span class="m">100&lt;/span> &lt;span class="m">100000&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Hadoop 的命令类似 Java 命令，通过 jar 指定要运行的程序所在的 jar 包 hadoop-mapreduce-examples-2.10.1.jar。参数 pi 表示需要计算的圆周率 $ \pi $。后面两个参数中，100 是指要运行 100 次 map，100000 表示每个 map 的任务次数，即每个节点要模拟飞镖 100000 次。执行过程及结果如下图：&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20200926210747.png"
loading="lazy"
alt="20200926210747"
>&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/jinggqu/blog_images@main/20200926211331.png"
loading="lazy"
alt="20200926211331"
>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Job Finished in 131.634 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Estimated value of Pi is 3.14158440000000000000
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>至此，Hadoop 环境配置完成。&lt;/p>
&lt;h1 id="备注">备注&lt;/h1>
&lt;p>如果在执行 mapreduce 任务中报错如 &lt;a class="link" href="https://stackoverflow.com/questions/21005643/container-is-running-beyond-memory-limits" target="_blank" rel="noopener"
>此问题&lt;/a> 的描述，参考 &lt;a class="link" href="https://web.archive.org/web/20170610145449/http://hortonworks.com/blog/how-to-plan-and-configure-yarn-in-hdp-2-0/" target="_blank" rel="noopener"
>此篇文章&lt;/a>，需要在 mapred-site.xml 文件中添加下列配置：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>mapreduce.map.memory.mb&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>4096&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>mapreduce.reduce.memory.mb&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>8192&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>mapreduce.map.java.opts&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>-Xmx3072m&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;name&amp;gt;&lt;/span>mapreduce.reduce.java.opts&lt;span class="nt">&amp;lt;/name&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;value&amp;gt;&lt;/span>-Xmx6144m&lt;span class="nt">&amp;lt;/value&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/property&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item></channel></rss>