[{"content":"Proof You Can Do Hard Things And if you’re not someone who knows they can do hard things, find a way to prove it to yourself. Build a habit, learn a skill, create something, whatever it is that turns your default stance on challenges from “that seems hard” to “I can figure it out.”\nsource: https://blog.nateliason.com/p/proof-you-can-do-hard-things\nStop Obsessing Over Tools I often see people in a constant search for the best note taking app. Or the best Linux distro and desktop setup. Or the best AI tool to enhance productivity. Or the best game engine. And so on, you guys get the point.\nThis search takes you nowhere. Doing this doesn’t make you productive. You will never find the best or the most perfect setup. Settling for good enough is most often the best thing you do. Otherwise you’ll find yourself productive in the search of being productive.\nsource: https://plug-world.com/posts/stop-obsessing-over-tools/\n重来：更为简单有效的商业思维 From chapter 音乐就在你的指尖流淌\n吉他大师说：“音乐就在你的指尖流淌。”就算你买了和艾迪·范·海伦（Eddie Van Halen）一样的吉他、效果踏板、扩音器，但是当你用这套装备来演奏时，弹出来的依然是你自己的风格。\n同理，给艾迪配一套从当铺倒腾出来的劣质装备，人家一出手，你还是能听出是艾迪·范·海伦的水平。好的装备的确能带来一些帮助，但事实是，你的演奏水平是由你自己的手指决定的。\n人们总忍不住要执着于工具，而不关注要用这些工具去做的事情。你见过这类人：能玩转一大堆震撼的艺术字体和漂亮的 Photoshop 滤镜效果的设计师，却不知道该表达什么。业余摄影爱好者总为使用胶片相机还是数码相机而争论不休，却没人关注拍出绝妙照片的决定因素是什么。\n很多业余高尔夫球手执着于加入昂贵的俱乐部，但是真正重要的是如何挥杆，而不是加入哪个俱乐部。就算让老虎伍兹加入廉价的俱乐部，他也照样能摆平你。\n人们把装备当作取胜的法宝，却不愿意花时间去练习，于是一直泡在专业器材店里。他们想要寻找捷径，然而，最好的工具不是用在普通领域的。而且你在起步阶段肯定用不上它。\n在商业领域，太多人纠结于工具的好坏、软件技巧、规模大小、舒适的办公环境、豪华的家具以及其他浮华的东西，而不去关注真正的要点。真正的要点就是怎样赢得客户、如何赢利。\n我们还可以看到一些人想要通过博客、播客或拍摄纪录片来宣传他们的业务，却受困于不知选择什么工具。真正要紧的是宣传的内容。你可以花大手笔购买超级棒的设备，但是如果没有什么内容可表达……那么，你还真没什么可说的。\n就用你现在手头有的或者能负担得起的，然后开始做吧。工具不重要，就用现有的工具也可以做得一样棒，音乐就在你的指尖流淌。\nsource: https://book.douban.com/subject/30184215/\n","date":"2023-11-13T21:00:00+08:00","permalink":"https://jinggqu.github.io/posts/a-few-thoughts/","title":"A Few Thoughts"},{"content":"前言 项目完整代码见 GitHub 仓库： jinggqu/MLNetDemo。\n本文将介绍如何使用 C#、WPF 与 ONNX 模型实现一个简单的手写数字识别项目。整个流程跑通后，即可应用更加复杂的深度学习模型。\nWPF 界面开发 项目的用户界面相对简单，主要包含一个 InkCanvas（Name 设置为 inkCanvas） 用于用户绘制数字，一个识别按钮与一个清除 Canvas 内容的按钮。软件用户界面如下图所示。\n项目最终效果图如下图所示。\nWPF 界面采用 xaml 文件进行定义，项目界面源代码详见 GitHub 仓库。\nONNX ONNX 简介 ONNX 是一种用于表示机器学习模型的开放格式。ONNX 定义了一组通用运算符（机器学习和深度学习模型的构建基块）和通用文件格式，使 AI 开发人员能够使用具有各种框架、工具、运行时和编译器的模型。详见ONNX 官方网站。\nONNX 模型库及项目模型选用 ONNX Model Zoo 收录了大量的预训练模型，包括计算机视觉领域常用的目标检测、图像分类及自然语言处理领域的 GPT 模型。处于演示需要，本项目选择 ONNX Model Zoo 中的手写数字识别模型作为实验模型。\n查看模型结构 下载 ONNX 模型后，我们还需要找到模型中的输入变量名与输出变量名。因此使用 Netron 来检视模型。Netron 打开 ONNX 模型后的效果如下图所示。\n从图中右侧可以看到，模型输入变量名为 Input3，数据类型为 float 数组，尺寸为 1×1×28×28。输出变量名为 Plus214_Output_0，数据类型为 float 数组，尺寸为 1×10，即为十分类中的每个类别概率值。\n整合模型 Visual Studio 安装深度学习插件 针对不同的平台，可以在 ONNX Runtime 官网选择对应的插件或依赖，详见ONNX Runtime。\n本项目采用 Visual Studio 2022 开发，按照官方说明需要安装 Microsoft.ML.OnnxRuntime，读者可以在 Visual Studio 中解决方案资源管理器中右键单击解决方案名称，选择管理 NuGet 程序包选项，搜索安装 Microsoft.ML.OnnxRuntime。\n除了安装上述插件外，还需要安装微软开发的应用于.Net 平台的机器学习开发包，同时本项目还涉及到图像处理，因此也需要安装图像处理相关的包。故需要安装的所有依赖包如下：\nMicrosoft.ML Microsoft.ML.OnnxRuntime Microsoft.ML.OnnxTransformer System.Drawing.Common 输入输出数据定义 对于输入数据，通过 Netron 得到输入变量名和数据类型后，即可得到如下的输入数据定义。\n1 2 3 4 5 6 public class InputData { [VectorType(1 * 1 * 28 * 28)] [ColumnName(\u0026#34;Input3\u0026#34;)] public float[] Image { get; set; } } 其中 VectorType 用于表征数据尺寸，由于本项目不涉及输入批次，仅为单张图片输入模型，因此第一维的 Batch Size 设置为 1，第二维的通道数也设置为 1。因此省略前两个维度，写成 [VectorType(28 * 28)] 也是可以的。ColumnName 需要与上述 Netron 中展示的输入变量名严格对应。\n对于输出数据，同上可得到如下的数据定义，不再赘述。\n1 2 3 4 5 public class OutputData { [ColumnName(\u0026#34;Plus214_Output_0\u0026#34;)] public float[] Result { get; set; } } C# 图像处理 本项目涉及的图像处理总体流程：\n获取 InkCanvas 的内容并转为位图 Bitmap 将 Bitmap 从原始尺寸变换到到模型输入规定的尺寸 将变换尺寸后的 Bitmap 转为单通道 8 位灰度图 将 Bitmap 对象转为 float 一维数组 获取 InkCanvas 的内容并转为位图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 private float[] ConvertInkCanvasToFloatArray() { // 获取InkCanvas的大小 int width = (int)inkCanvas.ActualWidth; int height = (int)inkCanvas.ActualHeight; // 创建RenderTargetBitmap RenderTargetBitmap rtb = new RenderTargetBitmap(width, height, 96, 96, PixelFormats.Default); rtb.Render(inkCanvas); // 转换为8位灰度图 FormatConvertedBitmap grayscaleBitmap = new FormatConvertedBitmap(); grayscaleBitmap.BeginInit(); grayscaleBitmap.Source = rtb; grayscaleBitmap.DestinationFormat = PixelFormats.Gray8; // 8-bit grayscale grayscaleBitmap.EndInit(); // 将WPF的BitmapSource转换为System.Drawing.Bitmap Bitmap bitmap; using (MemoryStream outStream = new MemoryStream()) { BitmapEncoder enc = new BmpBitmapEncoder(); enc.Frames.Add(BitmapFrame.Create(grayscaleBitmap)); enc.Save(outStream); bitmap = new Bitmap(outStream); } // 变换 Bitmap 尺寸 Bitmap bmp = ReSizeImage(bitmap, inputWidth, inputHeight); // 通道变换，获取单通道灰度图 Bitmap graybmp = GetGaryImage(bmp); // 将图像转为一维数组并返回 return ConvertBitmapToFloatArray(graybmp); } 变换 Bitmap 尺寸 本项目中画布的尺寸为 336×336，但手写数组识别模型使用的训练数据集为 MNIST，从模型输入数据中可以看到其图像尺寸为 28×28，因此需要变换位图的尺寸。\n1 2 3 4 5 6 7 8 9 private static Bitmap ReSizeImage(Image img, int width, int height) { Bitmap bitmap = new Bitmap(width, height); Graphics g = Graphics.FromImage(bitmap); g.InterpolationMode = InterpolationMode.HighQualityBicubic; g.DrawImage(img, 0, 0, bitmap.Width, bitmap.Height); g.Dispose(); return bitmap; } 通道变换 由于上述过程中生成的图像为 RGB 图像，但本例仅需要单通道灰度图，即与 MNIST 数据集保持一致，因此需要对其进行通道变换。此处选用 GDI+ 的 ColorMatrix 特性实现通道变换，代码参考自ML.NET (。・∀・)ノ 来用 C# 跑机器学习吧!。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 private static Bitmap GetGaryImage(Bitmap src) { float[][] colorMatrix = { new float[] {0.299f, 0.299f, 0.299f, 0, 0}, new float[] {0.587f, 0.587f, 0.587f, 0, 0}, new float[] {0.114f, 0.114f, 0.114f, 0, 0}, new float[] { 0, 0, 0, 1, 0}, new float[] { 0, 0, 0, 0, 1} }; ImageAttributes ia = new ImageAttributes(); ColorMatrix cm = new ColorMatrix(colorMatrix); ia.SetColorMatrix(cm, ColorMatrixFlag.Default, ColorAdjustType.Bitmap); Graphics g = Graphics.FromImage(src); g.DrawImage( src, new Rectangle(0, 0, src.Width, src.Height), 0, 0, src.Width, src.Height, GraphicsUnit.Pixel, ia ); g.Dispose(); return src; } 变换后得到的灰度图如下图所示（28×28，图片显示效果较小）。\n将图像转为一维数组 根据上述分析，输入数据为一维 float 数组，因此还需要将灰度图转换为一维数组。\n1 2 3 4 5 6 7 8 9 10 11 12 13 private float[] ConvertBitmapToFloatArray(Bitmap graybmp) { float[] graydata = new float[inputWidth * inputHeight]; for (int i = 0; i \u0026lt; inputWidth; i += 1) { for (int j = 0; j \u0026lt; inputHeight; j += 1) { System.Drawing.Color rescolor = graybmp.GetPixel(j, i); graydata[(i * inputWidth) + j] = rescolor.R / 255.0f; } } return graydata; } 初始化模型 首先定义两个全局变量 _modelPath 和 _predictionEngine，分别代表 ONNX 的模型存放地址与 TTransformer 模型推理变量。在初始化模型时，加载模型后给模型传入一组空输入参数以创建推理变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 private readonly string _modelPath = \u0026#34;../../../assets/mnist.onnx\u0026#34;; private PredictionEngine\u0026lt;InputData, OutputData\u0026gt; _predictionEngine; private void InitializeModel() { MLContext context = new MLContext(); var pipeline = context.Transforms.ApplyOnnxModel(_modelPath); var emptyData = new List\u0026lt;InputData\u0026gt;(); var data = context.Data.LoadFromEnumerable(emptyData); var model = pipeline.Fit(data); _predictionEngine = context.Model.CreatePredictionEngine\u0026lt;InputData, OutputData\u0026gt;(model); } 清除画布 为了清除画布，需要为 WPF 布局中的 Clear Digit 按钮绑定名为 ClearButtonClick 的事件，事件实现如下。其中，numberLabel为显示识别结果的标签，清除画布时需要同时将其置空。\n1 2 3 4 5 private void ClearButtonClick(object sender, RoutedEventArgs e) { inkCanvas.Strokes.Clear(); numberLabel.Text = \u0026#34;\u0026#34;; } 预测结果 在上述图像处理的基础上，调用模型预测结果之前还需要将图像处理结果传给模型，同时为 recognize 按钮绑定 RecognizeDigit。其中，numberLabel为显示识别结果的标签，因此需要将模型预测结果（1×10 个概率值）中最大的概率值所代表的数字，赋值给 numberLabel。\n1 2 3 4 5 private void RecognizeDigit(object sender, RoutedEventArgs e) { var result = _predictionEngine.Predict(new InputData() { Image = ConvertInkCanvasToFloatArray() }); numberLabel.Text = result.Result.ToList().IndexOf((float)result.Result.Max()).ToString(); } 参考文献 ML.NET (。・∀・)ノ 来用 C# 跑机器学习吧! 使用 ML.Net 轻松接入 AI 模型！ Chat GPT 4 Tutorial: Create a Windows Machine Learning UWP application (C#) ","date":"2023-08-18T15:00:00+08:00","permalink":"https://jinggqu.github.io/posts/handwritten-digits-recognition-using-csharp-wpf-onnx/","title":"使用C#、WPF与ONNX模型实现手写数字识别"},{"content":"笔者使用六月一整个月时间备考，平均每天 4 小时，备考后半程几乎每天都会严格按照雅思考试的时间限制做一套雅思真题。7 月 10 号参加了雅思考试，最终取得如下成绩。\n听力 阅读 写作 口语 总成绩 7.5 7.5 6.0 6.5 7.0 本文将详细介绍个人的备考雅思经历，以及如何在短时间内达到目标分数。\n背景 笔者在 2018 年 6 月通过了大学英语六级考试，获得 524 分（听力 198 分，阅读 202 分，写作和翻译 124 分。写作部分可以说非常拉跨了……），此后再未参加过专业英语考试。笔者平时有观看收听英语视频、影视剧及音乐的习惯，因此单词量基本维持在合理水平，自测单词量保持在 6000~7000 左右。读者可通过以下网站测试单词量，获取自己的词汇大致水平，以便对后续的复习计划做出相应调整。\n扇贝词汇量测试 Preply - Test your English vocabulary 重要前提 雅思，全称 International English Language Testing System，简写为 IELTS，是评估非英语母语人士的英语语言能力的一套国际标准化测试系统。既然它是一套标准化测试系统，那么读者就应当弄清楚系统的运作方式，方能知己知彼百战不殆。\n评分标准 雅思详细的评分标准可见其官方网站：雅思评分标准介绍。\n雅思题型可见官网介绍雅思题型介绍。\n听力与阅读部分的计分规则可见 新东方雅思考试评分标准对照表。\n值得注意的是，通过四部分分数计算雅思总成绩时，遵循四舍五入到最近 0.5 分的原则，例如：\n序号 听力 阅读 写作 口语 总分 平均分 总成绩 1 7 6.5 6 5.5 25 6.25 6.5 2 7 7 6 6.5 26.5 6.625 6.5 3 7.5 7.5 6 6.5 27.5 6.875 7.0 由于这种规则的特殊性，我们可以适当“投机取巧”达到目标分数，如例 1 中的均分仅为 6.25，最终总成绩为 6.5（其实这也是笔者原定的各科目标及总分）。即如果读者需要总成绩 6.5 分，那么实际上达到平均 6.25 即可；类似的，如果需要总成绩 7.0，那么平均分达到 6.75 即可。读者可以通过以下网站计算分数，以便于合理规划各考试部分的目标分数。\nOverall Band Score Calculator IELTS Band Score Calculators 单词量 又到了老生常谈的背单词环节。作为一门语言的基石，单词的重要性不言而喻。关于如何背单词，笔者已有的经验主要包括以下两点：\n采取“大水漫灌”策略。笔者认为，每天仅仅背 50 个甚至 20 个等小数量的单词意义不大，不仅不能有效复习前一天的单词，也不能学习到足量的新词，实属“食之无味，弃之可惜”。笔者在备考期间，每天花费大约 40~60 分钟记单词，每日总单词量约 240 个，其中包含 180 个之前学过的旧词，以及 60 个新词。 配合例句食用。如果仅仅记忆一个孤零零的单词，不仅记忆难度大，也容易遗忘。试想一下，我们在学习汉学到新的字或词后，第一反应都是使用这些词造句，以加深记忆。同理，学习英语单词时，我们也应该记忆配合该单词出现的例句。切勿使用不附带例句的背单词软件或应用。 相关资料与应用 笔者在复习期间使用的相关应用如下：\n背单词：扇贝单词网页版。扇贝单词网页版可使用键盘快捷操作，同时在选择不认识当前单词后，首先跳出来的并不是单词的汉语意思，而是单词例句，这是笔者认为这款产品最优秀的亮点。 真题模拟网页端：新东方雅思机考。没什么好说的，直接吹爆。软件界面和考试真机界面保持一致，且免费使用，模考后不仅自动打分，还可看到答案与解析，方便听力阅读和作文重听与改错。 逐个击破 雅思考试的四个部分包括听说读写，我们可以将其归类为：输入（听力和阅读）与输出（口语和写作）。一般情况下，输出能力建立在充足的输入信息的基础上，因此如何输入以及大量输入至关重要。\n听力 笔者身边很多朋友反馈说听力很是头疼，听不懂别人在说什么，也不知道如何提高。实际上，唯一的办法就是多听。根据笔者自己的一些经验，想要提升听力主要可以分为以下两个方面。\n日常会话听力 日常会话听力是指随机性较强、没有经过特殊编排、且不需要每个字每个词都完全听得清楚的听力内容，例如英语播客、自媒体视频以及流行音乐等。面对这些听力材料，即使我们并不能听懂所有的内容，但仍然可以根据上下文内容进行合理猜测，以便理解大意。同时，这些材料我们也不需要反反复复精听。在日常生活中，笔者基本上每天都会接触一些英语内容，包括自媒体视频、影视剧等内容，主要以兴趣为导向。为抛砖引玉，笔者在此处推荐一些个人常看常听的英语内容：\n哔哩哔哩 - 汤圆学英语 哔哩哔哩 - 瑞秋英语 Rachel 哔哩哔哩 - polyglot_maniac 哔哩哔哩 - 王有菜_ 哔哩哔哩 - LinusTechTips 哔哩哔哩 - Chubbyemu YouTube - BBC Learning English YouTube - Kurzgesagt YouTube - Beau Miles YouTube - Cynthia Zhou 应试听力 相较于日常会话听力的轻松随意，应试听力就显得有些“刻意而为之”。因此，针对应试听力材料，我们应该采用三步走策略，具体步骤如下：\n第一次听：提前快速浏览阅读听力题目与选项，标注题目或选项中的关键词，全神贯注聆听录音并做出选择。做出选择后，除非时间特别富裕或者特别确定选错了，否则不要回头更改答案； 第二次听：重新播放听力材料，重新仔细阅读错题题目和选项，重新播放对应部分听力材料再次作答； 第三次听：在上述两个步骤之后，如果还有错题，那么可以查看答案、听力原文或者题目解析，再次播放对应部分听力材料，在搞懂正确选项的同时总结错听、漏听的原因。 上述三步走策略是笔者从高中考试就一直应用的一种听力备考方法，虽然没能取得特别高的听力分数，但是至少可以确保听力不会成为短板项目，因此建议读者参考并按照自己的实际情况做出调整。\n阅读 与听力类似，阅读也可以采用类似的三步走策略。但是在雅思考试中，总共有三篇文章、40 个题目需要作答，而阅读部分的总时间仅为 60 分钟，这就非常考验我们的阅读能力以及快速寻找信息的能力。\n因此，在有限时间内突破雅思阅读通常依赖关键词法与平行阅读法。通常情况下，不同题型之间可能并不是按照文章段落先后顺序排布，但是每一种题型内部（如 T/F/NG 判断题、关键词填空题、人名材料匹配题等）通常是有序的。因此，我们可以并行多种题型，同时在文中找出与题干相关联的关键词，以达到最大效率。\n平行阅读法及相关资料可见：\n新东方在线雅思阅读技巧：平行阅读法 Simon 雅思阅读课程（请读者务必看一遍，哪怕 2 倍速也要看一遍） 写作 写作是笔者花费时间最多、考得最烂的部分，但还是有一些经验可以分享。写作分为小作文和大作文，小作文是看图表做事实写作，无需发表主观意见，描述客观事实即可；大作文是针对题目所给的话题做议论写作，通常是一些社会话题，例如对大学入学前 GAP 一年的看法、对政府大力投资航空航天的看法等，需要旗帜鲜明地写出观点，并举例以支撑提出的观点。\n时间分配方面，小作文通常安排 20 分钟，大作文 40 分钟。内容方面，小作文最少需要 150 词，大作文最少需要 250 词。根据 Simon 雅思作文思想，笔者的大小作文均采用四段式书写结构，简而言之可以归纳为如下：\n小作文：题干转述（1~2 句）、总体段（1~2 句）、主要对比特征一（3~4 句）、主要对比特征二（3~4 句）\n大作文：题干转述 + 自己观点（2~3 句）、正面观点并举例（5 句）、反面观点并举例（5 句）、结论（2~3 句）\n参考资料方面，笔者主要使用了以下参考资料：\nSimon 雅思作文课程。Simon 的课程详细讲解了作文的主题结构规划以及行文逻辑，一定要多看几遍； 慎小嶷 - 十天突破雅思写作。十天突破雅思写作这本书，给出了大量观点语料，同时还提供了适当的观点例子。书中包含大量的真题作文高分范文，并做了详细的点评分析，建议读者通读一遍，牢记语料库中的观点语料，避免考试时大脑宕机一片空白。 由于笔者作文分不高，因此在做题技巧方面没有太多可分享的内容，但请读者一定要多加练习，并严格遵循考试时间限制。\n口语 口语是最出乎笔者意料的一部分，在备考时笔者曾购买过两次雅思口语模考一对一课程，两次课程的老师都给了笔者口语 5.5，但最终在考试中居然拿到了 6.5，真是不可思议。口语主要分为以下三部分：\n在 Topic 1 中，雅思口语考官会提出多个较为基础的问题，针对每个问题考生都应该简要回答，通常是 2~3 句话。通常围绕你所居住的城市、家乡、学习或工作等常见话题；\n在 Topic 2 中，雅思口语考官会给出一个话题，考生要围绕话题引出故事、回答主要问题、事后感受等内容，回答时间不超过 2 分钟，超时会被考官打断；\n在这一部分，模考老师给了一个回答模板，读者可以参考。\n例如题目：\nDescribe a place away from your home and you want to visit in the future\nYou should say:\nWhere you would like to go\nWhen you would like to go\nWho do you want to go with\nAnd explain why you want to visit the place\n回答模板主要包括以下部分\nIntroduction: I\u0026rsquo;d like to talk about the city away from home that I want to visit in the future \u0026hellip;\nMain points: 回答上述题目中提出的问题\nFuture / Feelings: 描述对未来的规划或事后的感受\n在 Topic 3 中，雅思口语考官会针对 Topic 2 中的话题延申提问，同时还有可能提一些更加宏观的问题，例如儿童应该从传统中学到什么、邻居是否应该互相帮助、人们是否应该多买本国生产的商品等等。\n雅思口语题库通常会在 1、5、9 月更换，在备考口语时应该以当季题库作为基准，熟练掌握必备题，扩展掌握当季新题。在备考时，一定不要背诵答案，要按照话题自己给自己提问，然后按照自己的思路组织语言回答。\n读者可以自行搜索“雅思口语题库”拿到当季题库。例如，2023 年 5~8 月的题库可见新东方在线 - 2023 年雅思口语题库 5 月-8 月完整版（含答案）汇总。\n值得注意的是，不论是机考（视频通话形式）还是真人考官面对面口语考试，可以想象成和一位讲英语的朋友面对面聊聊天，这样就可以快速放松下来。在整个过程中，考生一定要充满自信，同时要和考官保持适当的眼神接触。\n结语 雅思考试分为纸笔考试和上机考试两种模式，纸笔考试出分慢（2~3 周），机考出分快（3 个自然日）。时至今日，除非读者不能熟练操作鼠标和键盘，笔者都建议参加机考。抛开出分的快慢不谈，就只针对机考写作部分可以非常方便地修改调整，并实时显示词数这一优点而言，笔者认为选择机考可以在很大程度上提高雅思作答效率。\n最后，下面这句话送给阅读本文的读者，预祝雅思考试顺利。\n纸上得来终觉浅，绝知此事要躬行\n","date":"2023-07-12T19:00:00+08:00","permalink":"https://jinggqu.github.io/posts/ielts-7.0/","title":"一个月突破雅思 7.0"},{"content":"第一题 题目描述 小美在摆弄她的字符串。最近小团送了小美一个特殊字符 \u0026lsquo;*\u0026rsquo;，这个字符可以和其他所有字符匹配，除了这个字符外，其他字符只能自己和自己匹配。小美先前有一个字符串 S，长度为 n，现在她又新组合了一个可有特殊字符 \u0026lsquo;*\u0026rsquo; 的字符串 s，长度为 m。小美想知道有多少个位置 i，使得 S[i+j] 与 s[j] 对于 1≤j≤m 均能匹配上？其中 X[y] 代表字符串 X 中第 y 位的字符。\n测试样例 第一行两个空格隔开的正整数 n 和 m，分别表示字符串 S 和字符串 s 的长度；\n接下来一行长度为 n 的仅包含小写英文字母的字符串 S；\n接下来一行长度为 m 的包含小写英文字母以及特殊字符 \u0026lsquo;*\u0026rsquo; 的字符串 s；\n对于所有数据，1≤m≤n≤2000，输出一行一个整数，表示满足要求的位置数量\n测试样例 1\n输入\n7 3\nabcaacc\na*c\n输出\n3\n样例 1 解释\n可以对 i=0,3,4 这三个位置的子串 abc、aac、acc 匹配上 a*c，即\nabcaacc\nabcaacc\nabcaacc\n思路与代码 模拟即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); char[] S = in.next().trim().toCharArray(); char[] s = in.next().trim().toCharArray(); if (n == m) { System.out.println(Arrays.equals(S, s) ? 1 : 0); return; } int res = 0; for (int i = 0; i \u0026lt;= n - m; i++) { if (S[i] == s[0] || s[0] == \u0026#39;*\u0026#39;) { int j = 0; for (; j \u0026lt; m; j++) { if (s[j] == \u0026#39;*\u0026#39;) continue; if (S[i + j] != s[j]) break; } if (j == m) res++; } } System.out.println(res); } } 第二题 题目描述 小美有一个精致的珠宝链子。初始这个链子上有 n 个宝石，从左到右分别编号为 1~n（宝石上的编号不会因为交换位置而改变编号）。接着，小美为了美观对这个项链进行微调，有 m 次操作，每次选择一个编号 x ,将编号 x 的宝石放到最左边（不改变其他宝石的相对位置）。小美想知道，所有操作完成后最终链子从左到右宝石编号是多少。\n测试样例 第一行两个正整数 n 和 m，分别表示链子上的宝石数和操作次数。\n接下来一行 m 个数 x1,x2,\u0026hellip;,xm，依次表示每次操作选择的编号 x 值。\n数字间两两有空格隔开\n对于所有数据，1≤m,n≤50000, 1≤xi≤n，输出一行 n 个整数，表示答案。\n测试样例 1\n输入\n5 3\n2 3 4\n输出\n4 3 2 1 5\n样例 1 解释\n第一次微调完，链子为 2 1 3 4 5\n第二次微调完，链子为 3 2 1 4 5\n第三次微调完，链子为 4 3 2 1 5\n思路与代码 语法题，使用双端队列模拟即可。将指定的宝石从链子中移除，并将其添加到链子头部。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import java.util.ArrayDeque; import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); int[] ops = new int[m]; for (int i = 0; i \u0026lt; m; i++) { ops[i] = in.nextInt(); } ArrayDeque\u0026lt;Integer\u0026gt; deque = new ArrayDeque\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { deque.offerLast(i + 1); } for (int op : ops) { deque.remove(op); deque.offerFirst(op); } for (int i = 0; i \u0026lt; n; i++) { System.out.printf(\u0026#34;%d \u0026#34;, deque.removeFirst()); } } } 第三题 题目描述 小团最近获得了美美团团国的裁缝资格证，成为了一个裁缝！现在小团有一个长度为 n 的大布料 S（在这个国家布料其实是一个仅包含小写英文字母的字符串），小团可以将布料在任意位置剪切，例如布料 abcd 可以被裁剪为 a 与 bcd 或 ab 与 cd 或 abc 与 d，不过，裁剪完之后是不能拼接起来的，因为小团还不是很擅长拼接布料。现在小团想知道能不能有一种裁剪方式能让他把布料恰好裁剪成客人要求的小布料。形式化地，有一个串 S，问是否能将其划分成 m 个不相交的连续子串，使得这些连续子串可以与要求的连续子串一一对应。两个串相对应是指这两个串完全相等。例如\u0026quot;aab\u0026quot;=\u0026ldquo;aab\u0026rdquo; 但 \u0026ldquo;aab\u0026rdquo;≠\u0026ldquo;baa\u0026rdquo;。\n测试样例 第一行两个空格隔开的正整数 n 和 m，分别表示大布料 S 长度和客人要求的小布料数量。\n接下来一行一个长度为 n 的仅包含小写英文字母的串 S，表示大布料的组成。\n接下来一行 m 个空格隔开的数 x1,x2, \u0026hellip;,xm，依次表示所要求的小布料长度。\n接下来开始 m 行，每行一个长度为 xi 的仅包含小写英文字母的串 si，表示第 i 个小布料的组成。\n如果存在这样的方案，输出方案总数。如果不存在任何方案，输出 0。\n两个方案 A、B 不相同当且仅当方案 A 中存在一个相对于原始长布料的裁剪位置 i，而方案 B 中并未在该位置 i 裁剪。\n例如 aaaaaa 裁剪方案 aaa|aaa 与方案 aaa|aaa 是相同的方案。而方案 aa|aaaa 与方案 aaaa|aa 是不同的方案，\n虽然划分出的结果都是 aa 与 aaaa，但前者在第二个 a 处进行了裁剪，后者并没有在这里进行裁剪，所以视为不同的裁剪方案。\n测试样例 1\n输入\n6 2\naaaaaa\n4 2\naaaa\naa 输出\n2\n样例 1 解释\n有两种方案，第一种是 aaaa|aa，第二种是 aa|aaaa，代表一次裁剪。\n思路与代码 根据子集/排列思想，本题元素可重但不可复选。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 public class Main { static int res = 0; public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); in.nextLine(); String S = in.nextLine().trim(); in.nextLine(); String[] fragments = new String[m]; for (int i = 0; i \u0026lt; m; i++) { fragments[i] = in.next().trim(); } // 如果长度相同，则按照字典序排序，否则按照长度排序 Arrays.sort(fragments, ((o1, o2) -\u0026gt; { if (o1.length() == o2.length()) return o1.compareTo(o2); else return o1.length() - o2.length(); })); backtrack(S, new StringBuilder(), fragments, new boolean[m]); System.out.println(res); } private static void backtrack(String S, StringBuilder sb, String[] fragments, boolean[] used) { if (S.equals(sb.toString())) { res++; return; } for (int i = 0; i \u0026lt; fragments.length; i++) { // 元素可重，但不可复选 if (i \u0026gt; 0 \u0026amp;\u0026amp; fragments[i].equals(fragments[i - 1]) \u0026amp;\u0026amp; used[i - 1]) continue; if (used[i]) continue; sb.append(fragments[i]); used[i] = true; backtrack(S, sb, fragments, used); sb.replace(sb.length() - fragments[i].length(), sb.length(), \u0026#34;\u0026#34;); used[i] = false; } } } 第四题 题目描述 小团正忙着用机器人收衣服！因为快要下雨了，小团找来了不少机器人帮忙收衣服。他有 n 件衣服从左到右成一行排列，所在位置分别为 1~n，在每个位置上已经有一个就绪的机器人可以帮忙收衣服，但第 i 个位置上的机器人需要 pi 的电量来启动，然后这个机器人会用 ti 的时间收衣服，当它收完当前衣服后，会尝试去收紧邻的右边的一件衣服 (如果存在的话)，即 i+1 处的衣服，如果 i+1 处的衣服已经被其他机器人收了或者其他机器人正在收，这个机器人就会进入休眠状态，不再收衣服。不过如果机器人没有休眠，它会同样以 ti 时间来收这件 i+1 处的衣服（注意，不是 ti+1 的时间，收衣服的时间为每个机器人固有属性），然后它会做同样的检测来看能否继续收 i+2 处的衣服，一直直到它进入休眠状态或者右边没有衣服可以收了。形象地来说，机器人会一直尝试往右边收衣服， 收 k 件的话就耗费 k*ti 的时间，但是当它遇见其他机器人工作的痕迹，就会认为后面的事情它不用管了，开始摸鱼，进入休眠状态。小团手里总共有电量 b，他准备在 0 时刻的时候将所有他想启动的机器人全部一起启动，过后不再启动新的机器人， 并且启动的机器人的电量之和不大于 b。他想知道在最佳选择的情况下，最快多久能收完衣服。若无论如何怎样都收不完衣服，输出 -1。\n测试样例 第一行两个正整数 n 和 b，分别表示衣服数量和小团持有电量。\n接下来一行 n 个数 p1,p2, \u0026hellip;,pn，含义如题所述，为机器人唤醒需求电量。\n接下来一行 n 个数 t1,t2, \u0026hellip;,tn，含义如题所述，为机器人收衣服所需时间。\n数字间两两有空格隔开。\n输出最短所需时间。\n测试样例 1\n输入\n3 5\n1 2 3\n7 5 3\n输出\n10\n样例 1 解释\n可以同时启动第一个机器人和第二个机器人，耗电量为 1+2=3，这样花费时间为 max(7, 52)=10\n也可以同时启动第一个机器人和第三个机器人，耗电量为 1+3=4，这样花费时间为 max(72, 3)=14\n所以答案为 10\n测试样例 2\n输入\n3 5\n6 2 3\n7 5 3\n输出\n-1\n样例 2 解释 因为必须要启动第一个机器人，耗电量至少为 6，但是持有电量只有 5，因此无法收完所有衣服，输出 -1\n思路与代码 // TODO\n1 // TODO 第五题 题目描述 小美在回家路上碰见很多缠人的可爱猫猫！因为猫猫太可爱了以及小美十分有爱心，每遇到一只猫猫，小美忍不住停下来花费 T 的时间抚摸猫猫让猫猫不再缠着小美。而一路上小美能捡到很多亮闪闪的小玩具，这里我们给每个小玩具的种类都编了号，从 1~k，一共 k 种小玩具，对于每个所属种类 i 的小玩具，小美可以选择将它送给遇到的一只猫猫玩，这样的话可以只花费 ti 的时间就可以让这只猫猫心满意足的离开。小美想知道，在她以最佳的对小玩具的用法下，她最少耗费多少时间在打发猫猫（即只考虑摸猫时间以及用小玩具打发猫的时间）。注意，每个捡到的小玩具只能用一次。\n测试样例 第一行三个正整数 n、k 和 T，分别表示小美回家遇见的事件数、小玩具种类总数以及摸猫时间！\n接下来一行 k 个数 t1,t2, \u0026hellip;,tk, 含义如题所述，为每种小玩具打发猫猫所用时间。\n接下来一行 n 个数 e1,e2, \u0026hellip;,en，表示 n 次事件，对第 i 次事件，如果 ei=0，则表示遇到了一只猫猫，小美可以选择花费 T 的时间去抚摸，或者用一个小玩具送给猫猫来打发它 (如果小美有的话)。 如果 ei\u0026gt;0，则表示小美在这里捡到了一个小玩具，种类为 ei。初始时候小美身上没有任何小玩具，她可以携带任意多个小玩具。\n输出最少耗费多少时间在打发猫猫（即只考虑摸猫时间以及用小玩具打发猫的时间）。\n测试样例 1\n输入\n6 2 100\n1 50\n0 1 2 0 1 0\n输出\n102\n样例 1 解释\n一开始没有小玩具，遇到一只猫猫只能抚摸，花费了 100 的时间。\n接下来获得了小玩具 1 和 2，然后遇到一只猫猫，用了小玩具 1，只花费了 1 的时间。\n接下来又获得一个小玩具 1 之后又遇见一只猫猫，因为又有小玩具 1 了，所以还是只用花费 1 的时间。\n总共用时 102\n思路与代码 贪心法。使用 PriorityQueue 将所有的玩具打发猫猫的时间存储起来，如果玩具打发猫猫的时间大于抚摸猫猫的时间，则该玩具不要也罢。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), k = in.nextInt(), T = in.nextInt(); int[] t = new int[k], e = new int[n]; for (int i = 0; i \u0026lt; k; i++) { t[i] = in.nextInt(); } for (int i = 0; i \u0026lt; n; i++) { e[i] = in.nextInt(); } int res = 0; PriorityQueue\u0026lt;Integer\u0026gt; timeOfToys = new PriorityQueue\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { if (e[i] == 0) { if (timeOfToys.isEmpty()) res += T; else res += timeOfToys.poll(); } else { if (t[e[i] - 1] \u0026gt; T) continue; timeOfToys.add(t[e[i] - 1]); } } System.out.println(res); } } ","date":"2022-08-29T10:00:00+08:00","permalink":"https://jinggqu.github.io/posts/meituan-java-note-3/","title":"美团 Java 岗算法笔试记录（2022/08/27）"},{"content":"第一题 题目描述 小团想要自己来烤串！不过在烤串之前，需要串好烤串。小团有 n 个荤菜和 n 个素菜，他想按顺序分别一个荤菜一个素菜串起来，想请你帮他串好！给出两个长度分别为 n 的仅包含小写英文字母的串 A 和 B，分别代表荤菜和素菜的种类（用字母来表示菜的种类）。请你以从左到右的顺序依次串好他们！例如对于荤菜串 A1A2\u0026hellip;An 和素菜串 B1B2\u0026hellip;Bn，串好应该是 A1B1A2B2\u0026hellip;AnBn。\n测试样例 第一行一个正整数 n，表示烤串长度； 第二行为一个长度为 n 的字符串 A，表示荤菜按次序都是哪些菜； 第三行为一个长度为 n 的字符串 B，表示素菜按次序都是哪些菜； 对于 80% 的数据，n≤1000，对于 20% 的数据，n≤50000。于所有数据，A 和 B 为仅包含小写英文字母的字符串； 输出一行，包含 2n 个字符串表示串好的烤串。\n测试样例 1\n输入\n4\nabcd\nefgh\n输出\naebfcgdh\n思路与代码 语法题，直接交替插入两个字符串的字符即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); in.nextLine(); String A = in.nextLine(); String B = in.nextLine(); StringBuilder builder = new StringBuilder(); for (int i = 0; i \u0026lt; n; i++) { builder.append(A.charAt(i)); builder.append(B.charAt(i)); } System.out.println(builder); } } 第二题 题目描述 小团在地图上放了三个定位装置，想依赖他们来进行定位！小团的地图是一个 n×n 的一个棋盘，他在 (x1,y1),(x2,y2),(x3,y3) xi,yi ∈ Z ∩ [1,n] 这三个位置分别放置了一个定位装置（两两不重叠）。然后小团在一个特定的位置 (a,b)a,b ∈ Z ∩[1,n] 放置了一个信标。每个信标会告诉小团它自身到那个信标的曼哈顿距离，即对 i=1,2,3 小团知道 (|xi-a|+|yi-b|)，现在小团想让你帮他找出信标的位置！注意，题目保证最少有一个正确的信标位置。因为小团不能定位装置确定出来的信标位置是否唯一，如果有多个，输出字典序最小的那个。(a,b) 的字典序比 (c,d) 小，当且仅当 a\u0026lt;c 或者 a==c∧b\u0026lt;d。\n测试样例 第一行一个正整数 n，表示棋盘大小；\n第二行两个整数，分别表示 x1 与 y1，即第一个定位器的位置；\n第三行两个整数，分别表示 x2 与 y2，即第二个定位器的位置；\n第四行两个整数，分别表示 x3 与 y3，即第三个定位器的位置；\n第五行三个整数，分别表示第一、二、三个定位器到信标的曼哈顿距离。第 i 个定位器到信标的曼哈顿距离即 (|xi-a|+|yi-b|)；\n数字间两两有空格隔开，对于所有数据，n≤50000, 1≤xi,yi≤n，输出一行两个整数，表示字典序最小的可能的信标位置。\n测试样例 1\n输入\n3 2 1 2 2 2 3 2 1 2\n输出\n1 2\n样例 1 解释\n与 (2, 1) 的哈曼顿距离为 2 的位置有三个，分别是 (1, 2), (2, 3), (3, 2)\n与 (2, 2) 的哈曼顿距离为 1 的位置有四个，分别是 (1, 2), (2, 1), (2, 3), (3, 2)\n与 (2, 3) 的哈曼顿距离为 2 的位置有三个，分别是 (1, 2), (2, 1), (3, 2)\n所以只有 (1, 2), (3, 2) 这两个位置有可能是信标，而 (1, 2) 的字典序最小，所以输出 (1, 2)\n思路与代码 曼哈顿距离即水平和竖直方向上的距离之和。使用 directions 表示方向，对距离 $d$ 进行遍历，若水平方向取距离 $d_x$，则竖直方向距离 $d_y = d - d_x$，枚举出所有点后判断点是否在边界内，若在则有效。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 public class Main { static int[][] directions = {{1, -1}, {1, 1}, {-1, 1}, {-1, -1}}; public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); int x1 = in.nextInt(), y1 = in.nextInt(); int x2 = in.nextInt(), y2 = in.nextInt(); int x3 = in.nextInt(), y3 = in.nextInt(); int d1 = in.nextInt(), d2 = in.nextInt(), d3 = in.nextInt(); Set\u0026lt;String\u0026gt; set1 = getAdjoin(n, x1, y1, d1); Set\u0026lt;String\u0026gt; set2 = getAdjoin(n, x2, y2, d2); Set\u0026lt;String\u0026gt; set3 = getAdjoin(n, x3, y3, d3); PriorityQueue\u0026lt;int[]\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;((a, b) -\u0026gt; { if (a[0] == b[0]) return a[1] - b[1]; else return a[0] - b[0]; }); for (String s : set1) { if (set2.contains(s) \u0026amp;\u0026amp; set3.contains(s)) { String[] pair = s.split(\u0026#34;-\u0026#34;); queue.offer(new int[]{Integer.parseInt(pair[0]), Integer.parseInt(pair[1])}); } } int[] pair = queue.poll(); System.out.println(pair[0] + \u0026#34; \u0026#34; + pair[1]); } static Set\u0026lt;String\u0026gt; getAdjoin(int n, int x, int y, int d) { Set\u0026lt;String\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); for (int i = d; i \u0026gt;= 0; i--) { for (int[] dir : directions) { int newX = x + i * dir[0], newY = y + (d - i) * dir[1]; if (newX \u0026gt;= 1 \u0026amp;\u0026amp; newX \u0026lt;= n \u0026amp;\u0026amp; newY \u0026gt;= 1 \u0026amp;\u0026amp; newY \u0026lt;= n) set.add(newX + \u0026#34;-\u0026#34; + newY); } } return set; } } 第三题 题目描述 小美即将进行期末考试！小美现在盘算了一下，一共有 n 道试题，对于第 i 道试题，小美有着 pi 的概率做对，获得 ai 的分值，另外 (1-pi) 的概率做错，获得 0 分。小美的总分即是每道题获得的分数之和。小美不甘于此！她决定突击复习，因为时间有限，她最多复习 m 道试题，使得复习后的试题正确率提升到 100%。小美想知道，如果她以最佳方式进行复习，能获得的期望总分最大是多少。\n测试样例 第一行两个正整数 n 和 m，表示总试题数和最多复习试题数。\n接下来一行 n 个整数，分别为 p1 p2\u0026hellip;pn，表示小美有 pi%的概率，即 pi=pi/100 的概率做对第 i 个题。（注意，这里为了简单起见，将概率 pi 扩张 100 倍成为整数 pi 方便输入）\n接下来一行 n 个整数，分别表示 a1 a2\u0026hellip;an，分别表示第 i 个题做对的分值。\n数字间两两有空格隔开，对于所有数据，1≤m≤n≤50000,0≤pi≤100,1≤ai≤1000\n输出一行一个恰好两位的小数，表示能获得的最大期望总分。（如果答案为 10 应输出 10.00，2.5 应输出 2.50）\n测试样例 1\n输入\n2 1\n89 38\n445 754 输出\n1150.05\n样例 1 解释\n如果都不复习，小美总分的期望为 89% * 445 + 38% * 754 = 682.57\n如果复习第一道题，小美总分的期望为 100% * 445 + 38% * 754 = 731.52\n如果复习第二道题，小美总分的期望为 89% * 445 + 100% * 754 = 1150.05\n所以选择复习第二道题，这样能获得最大期望总分 1150.05\n根据每题复习后的收益进行排序即可\n思路与代码 自定义数据结构 Pair，使用 PriorityQueue 对 Pair 按照收益倒序排列，即收益最大的课排在前。再通过贪心法，选取前 $m$ 门收益最大的课进行复习，剩余 $n-m$ 门课则不复习。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Main { static class Pair { int a; // 满分 double diff; // 满分与不复习得分的差值（收益） public Pair(int a, double diff) { this.a = a; this.diff = diff; } } public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); int[] p = new int[n], a = new int[n]; PriorityQueue\u0026lt;Pair\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;((o1, o2) -\u0026gt; { return o2.diff - o1.diff; }); for (int i = 0; i \u0026lt; n; i++) { p[i] = in.nextInt(); } for (int i = 0; i \u0026lt; n; i++) { a[i] = in.nextInt(); } for (int i = 0; i \u0026lt; n; i++) { queue.offer(new Pair(a[i], a[i] * (1 - p[i] / 100.0))); } double res = 0; for (int i = 0; i \u0026lt; n; i++) { Pair pair = queue.poll(); if (i \u0026lt; m) { res += pair.a; } else { res += pair.a - pair.diff; } } System.out.println(res); } } 第四题 题目描述 小团生日收到妈妈送的两个一模一样的数列作为礼物！他很开心的把玩，不过不小心没拿稳将数列摔坏了！现在他手上的两个数列分别为 A 和 B，长度分别为 n 和 m。小团很想再次让这两个数列变得一样。他现在能做两种操作，操作一是将一个选定数列中的某一个数 a 改成数 b，这会花费|b-a|的时间，操作二是选择一个数列中某个数 a，将它从数列中丢掉，花费|a|的时间。小团想知道，他最少能以多少时间将这两个数列变得再次相同！\n测试样例 第一行两个空格隔开的正整数 n 和 m，分别表示数列 A 和 B 的长度。\n接下来一行 n 个整数，分别为 A1 A2\u0026hellip;An\n接下来一行 m 个整数，分别为 B1 B2\u0026hellip;Bm\n对于所有数据，1≤n,m≤2000， |Ai|,|Bi|≤10。输出一行一个整数，表示最少花费时间，来使得两个数列相同。\n测试样例 1\n输入\n1 1\n-9821\n7742\n输出\n17563\n样例 1 解释\n可以选择两次第二种操作，消除数列 A 的第一个数和数列 B 的第一个数，需要花费 9821+7742=17563 的时间\n也可以选择一次第一种操作，将数列 A 的第一个数改成数列 B 的第一个数，也是需要花费 9821+7742=17563 的时间\n所以答案为 17563\n思路与代码 动态规划，与 leetcode 72.编辑距离 类似，但要注意设置本题的边界条件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); int[] A = new int[n]; int[] B = new int[m]; for (int i = 0; i \u0026lt; n; i++) { A[i] = in.nextInt(); } for (int i = 0; i \u0026lt; m; i++) { B[i] = in.nextInt(); } int[][] dp = new int[n + 1][m + 1]; // 丢掉 A[i] for (int i = 1; i \u0026lt;= n; i++) { dp[i][0] = dp[i - 1][0] + Math.abs(A[i - 1]); } // 丢掉 B[i] for (int i = 1; i \u0026lt;= m; i++) { dp[0][i] = dp[0][i - 1] + Math.abs(B[i - 1]); } for (int i = 1; i \u0026lt;= n; i++) { int a = A[i - 1]; for (int j = 1; j \u0026lt;= m; j++) { int b = B[j - 1]; if (a == b) { dp[i][j] = dp[i - 1][j - 1]; } else { dp[i][j] = Math.min(dp[i - 1][j - 1] + Math.abs(b - a), Math.min(dp[i - 1][j] + Math.abs(a), dp[i][j - 1] + Math.abs(b))); } } } System.out.println(dp[n][m]); } } 第五题 题目描述 小团的玩具火箭有点磨损了，上面有很多地方翘起来了，小团想要用强力胶进行修补，但在强力胶凝结之前，需要找点东西压住。幸好小团有很多这样的东西。小团有 m 种配重材料，第 i 种材料重 ai 单位重量（因为小团有太多了，可以认为每种都有任意多个）。火箭上有 n 个地方翘起来了，需要至少 bi 单位重量的东西来压住，而且只能用一个配重材料来压，(多了的话不好压，多个配重材料容易散开，所以小团不想用多个来折腾)。小团想一次就把所有翘起来的地方全都修补好，请问他需要使用的配重材料重量之和最少是多少？\n测试样例 第一行两个正整数 n 和 m，分别代表需要修补的地方个数以及材料种类数。\n接下来一行 n 个数 b1,b2,\u0026hellip;,bn，含义如题。\n接下来一行 m 个数 a1,a2,\u0026hellip;,am，含义如题。\n对于 40% 的数据，n,m≤100，对于另外 30% 的数据，n,m≤2000，对于所有数据，1≤n,m≤50000，1≤ai,bi≤104，输出小团最少需要使用的配重材料重量之和。如果没有任何办法满足，输出-1。\n测试样例 1\n输入\n1 1\n5\n4\n输出\n-1\n样例 1 解释\n需要 5 单位重量，只有 4 单位重量的材料，压不住，输出-1。\n测试样例 2\n输入\n3 3\n4 1 3\n4 2 1\n输出\n9\n样例 2 解释\n第一个地方需要重量为 4 的，第二个地方可以用重量为 1 的，第三个地方只能选择重量为 4 的才能压住。所以总重量需求为 9。可以证明没有更优方案。\n思路与代码 利用 TreeSet 的特性，遍历每个需要的重量 $b_i$，通过 ceiling(bi) 方法得到大于等于 $b_i$ 的最小值，若不存在这样的值则说明压不住，返回-1。或者对材料重量与所需重量排序后，基于上述思路二重遍历即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(); int[] b = new int[n]; TreeSet\u0026lt;Integer\u0026gt; ts = new TreeSet\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n; i++) { b[i] = in.nextInt(); } for (int i = 0; i \u0026lt; m; i++) { ts.add(in.nextInt()); } long res = 0; for (int bi : b) { Integer ceiling = ts.ceiling(bi); if (ceiling == null) { System.out.println(-1); return; } else { res += ceiling; } } System.out.println(res); } } 参考文献 1.08/20 美团后端笔试\n2.20220820 美团笔试题解\n","date":"2022-08-24T10:00:00+08:00","permalink":"https://jinggqu.github.io/posts/meituan-java-note-2/","title":"美团 Java 岗算法笔试记录（2022/08/20）"},{"content":"投递岗位 Java 开发工程师-China Geo\n一面（2022/08/16） 面试官自我介绍及团队业务介绍； 自我介绍； 项目相关问题； 逻辑题，在直角坐标系第一象限给定两个矩形，如何求出其相交面积？直播讲思路，写伪码； 线程的生命周期； 线程怎么跑起来的，为什么一个线程可以执行？ 线程都有哪些状态？什么时候 wait()，什么时候 sleep()？在写代码的时候会主动调用 wait() 吗？ Java 中常用的集合都有哪些？ HashMap 是什么结构？ HashMap 怎么实现的键值对？key 和 value 是什么结构？ HashMap 底层是怎么实现的； HashMap 为什么要使用红黑树； 什么是 O(1)、O(n)； 手写快排； 做项目的流程，在做项目的过程中有没有遇到问题； 是否可以接受工作中编码只占一小部分的情况； 评价与结语。 二面（2022/08/23） 自我介绍； 介绍项目，对负责的部分做详细的介绍； 从 Java 层面，如何做跨语言的程序调用； 假设有两台 Java 服务，如何互相做代码调用； 项目中常用的开发工具以及开发包都有什么？（Python 和 Java） 好的应用设计应该遵循什么标准； 常用的设计模式； 上述设计模式在哪些场景下使用； 代理模式在 Spring 里是怎样工作的； 常规 MVC 模式与 Spring MVC 模式有什么异同？ 项目中遇到技术难题，通过什么方式解决； 如何快速学习一门新的技术；（基于实践学习，自顶而下） 上述学习过程中，如何掌握新技术的基础知识； 讲讲 Java 内存回收；（对象生命周期分类，判断对象死亡方法，垃圾回收方法，垃圾回收器） 在实际项目中，应该在什么时候去设置 Java 堆栈大小？ 线下及线上服务的调试方法都有哪些，以及相应的工具； 未来的职业规划； 有没有什么想问的？（ChinaGeo 与 SSG 的区别，剩下几次面试，本次面试表现） ","date":"2022-08-16T19:00:00+08:00","permalink":"https://jinggqu.github.io/posts/lenovo-java-interview/","title":"联想面试记录（2022/08/23 更新）"},{"content":"第一题 题目描述 炸鸡店拥有一名会传送魔法的外卖派送员。该外卖派送员派送单子时，可以消耗时间 t 来正常派送单子（一次只能派送一个单子，不能多个同时派送），也可以使用魔法不耗费时间地隔空瞬间投送。现在炸鸡店在时刻 0 接收到了若干炸鸡订单，每个单子都有它的截止送达时间。外卖派送员需要保证送达时间小于等于这个截止时间。现在询问外卖员最少要使用几次魔法来保证没有外卖超时。\n测试样例 测试样例 1\n输入\n6 5\n5 6 7 8 9 10\n输出\n3\n测试样例 2\n输入\n6 5\n100 101 102 103 104 105\n输出\n0\n思路与代码 贪心算法。先排序，然后看看每一次送达时间是否能在截止时间前。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public static void main(String[] args) throws IOException { Scanner in = new Scanner(System.in); int n = in.nextInt(), t = in.nextInt(), time = 0; int[] deadline = new int[n]; for (int i = 0; i \u0026lt; n; i++) deadline[i] = in.nextInt(); Arrays.sort(deadline); int res = n; for (int i = 0; i \u0026lt; n; i++) { if (deadline[i] \u0026gt;= time) { res--; time += t; } } System.out.println(res); } 第二题 题目描述 你买了一个扫地机器人，你想要知道这个扫地机器人是否能够将房间打扫干净。为了简化问题，我们不妨假设房间被划分为 n*m 的方格。定义打扫干净为这 n*m 的方格全部被打扫过至少一次。你为扫地机器人下达了若干指令。每个指令为上下左右移动中的一种。机器人会将经过的路径上的方格打扫干净。初始时假设机器人处于第一行第一列的方格中。这个方格初始时会被机器人直接打扫干净。现在询问你机器人能否将房间打扫干净，能则输出 Yes，不能则输出 No。对于 Yes 的情况下，还要求你继续输出到哪个指令结束后，房间就打扫干净了。对于 No 的情况下，还要求你输出还有多少个地块没有打扫干净。保证机器人在打扫的过程中不会越过房间边界。换句话说机器人始终保持在 n*m 的方格图中。\n测试样例 测试样例 1\n输入\n2 2 5\n\u0026ldquo;SDWAS\u0026rdquo;\n输出\n\u0026ldquo;Yes\u0026rdquo;\n3\n测试样例 2\n输入\n2 2 5\n\u0026ldquo;SWSWS\u0026rdquo;\n输出\n\u0026ldquo;No\u0026rdquo;\n2\n思路与代码 暴力解法即可。无须考虑边界问题，因为题目已经保证机器人不会越界。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(), m = in.nextInt(), total = in.nextInt(); char[] orders = in.next().toCharArray(); boolean[][] cleaned = new boolean[n][m]; cleaned[0][0] = true; int x = 0, y = 0, remain = n * m - 1; for (int i = 0; i \u0026lt; total; i++) { char order = orders[i]; if (order == \u0026#39;W\u0026#39;) { x--; } else if (order == \u0026#39;A\u0026#39;) { y--; } else if (order == \u0026#39;S\u0026#39;) { x++; } else if (order == \u0026#39;D\u0026#39;) { y++; } if (!cleaned[x][y]) { remain--; cleaned[x][y] = true; } if (remain == 0) { System.out.println(\u0026#34;Yes\u0026#34;); System.out.println(i + 1); return; } } System.out.println(\u0026#34;No\u0026#34;); System.out.println(remain); } 第三题 题目描述 Alice 和 Bob 在玩一个游戏。有 n 张卡牌，点数分别为 1 到 n。进行洗牌后，n 张牌从上到下叠放形成一个牌堆。每次 Alice 先将当前牌堆顶的一张牌放到牌堆底，然后 Bob 再将当前牌堆顶的一张牌放到牌堆底。（特别地，当牌堆中只有一张牌时，相当于不进行任何操作）接着，他们会翻开当前牌堆顶的牌，并记下它的点数。当所有牌都被翻开后，他们也记下了 n 个点数。现在他们想根据记下的这个序列来还原一开始的牌（从牌堆顶到牌堆底每一张牌的点数）。\n测试样例 测试样例 1\n输入\n4\n1 2 3 4\n输出\n4 2 1 3\n样例 1 解释\n初始牌堆为：4 2 1 3\nAlice 和 Bob 分别操作后牌堆为：1 3 4 2，此时 1 被翻开，牌堆变为 3 4 2 Alice 和 Bob 分别操作后牌堆为：2 3 4，此时 2 被翻开，牌堆变为 3 4 Alice 和 Bob 分别操作后牌堆为：3 4，此时 3 被翻开，牌堆变为 4 Alice 和 Bob 分别操作后牌堆依旧为 4，此时 4 被翻开。 思路与代码 模拟倒推即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); int[] poker = new int[n]; for (int i = 0; i \u0026lt; n; i++) { poker[i] = in.nextInt(); } LinkedList\u0026lt;Integer\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(); // 题干操作顺序 // for (int i = 0; i \u0026lt; n; ++i) { // list.addLast(list.removeFirst()); // list.addLast(list.removeFirst()); // poker[i] = list.removeFirst(); // System.out.print(poker[i] + \u0026#34; \u0026#34;); // } for (int i = n - 1; i \u0026gt;= 0; i--) { list.addFirst(poker[i]); list.addFirst(list.removeLast()); list.addFirst(list.removeLast()); } for (int i = 0; i \u0026lt; n; i++) { System.out.printf(\u0026#34;%d \u0026#34;, list.get(i)); } } 第四题 题目描述 给一个长度为 n 的序列 a[1], a[2], …, a[n]，请问有多少个三元组 (i,j,k) 满足 i\u0026lt;j\u0026lt;k 且 a[i]-a[j]=2a[j]-a[k]？输出符合要求的三元组的数量。\n测试样例 测试样例 1\n输入\n4\n4 2 2 2\n输出\n3\n思路与代码 直接暴力解即可，注意数据范围（使用 long 而不是 int）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); long[] nums = new long[n]; HashMap\u0026lt;Long, List\u0026lt;Integer\u0026gt;\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { nums[i] = in.nextInt(); List\u0026lt;Integer\u0026gt; index = map.getOrDefault(nums[i], new ArrayList\u0026lt;\u0026gt;()); index.add(i); map.put(nums[i], index); } int count = 0; for (int i = 0; i \u0026lt; n; i++) { for (int j = i + 1; j \u0026lt; n; j++) { long target = -1 * (nums[i] - 3 * nums[j]); List\u0026lt;Integer\u0026gt; index = map.get(target); if (index != null) { for (Integer k : index) { if (k \u0026gt; j) count++; } } } } System.out.println(count); } 第五题 题目描述 给一棵有 n 个节点的二叉树，节点的编号从 1 到 n。其中，节点 k 的左儿子为节点 2*k（当 2*k 大于 n 时，不存在左儿子），节点 k 的右儿子为节点 2*k+1（当 2*k+1 大于 n 时，不存在右儿子），该二叉树的根节点为节点 1。对于每个节点，节点上有一些金钱。现在你可以从根节点 1 开始，每次选择左儿子或者右儿子向下走，直到走到叶子节点停止，并获得你走过的这些节点上的金钱。你的任务是求出你可以获得的最大的金钱总量。\n测试样例 第一行是一个正整数 n，表示二叉树上总共有 n 个节点。第二行有 n 个正整数，第 i 个正整数表示节点 i 上有多少数量的金钱。1 \u0026lt;= n \u0026lt;= 100000。对所有数据保证：单个节点上的金钱数量在 [1, 1000] 之间。\n测试样例 1\n输入\n3\n5 7 8\n输出\n13\n样例解释 1\n该样例中，二叉树上有三个节点，根节点为 1 号节点，其左儿子为 2 号节点，右儿子为 3 号节点，所能获取的最大金钱为 13，为从 1 号节点走到 3 号节点，共获得 5 + 8 = 13 的金钱。\n测试样例 2\n输入\n5\n863 163 396 428 90\n输出\n1454\n思路与代码 与 leetcode 124 类似，采用递归的思路，针对根节点 root，分别求得其左右子树中的路径之和较大者，再加上当前根节点 root 节点值（形成一个单臂，即根节点 + 左子节点，或根节点 + 右子节点），最后返回到上一层递归即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public static void main(String[] args) { Scanner in = new Scanner(System.in); int n = in.nextInt(); int[] arr = new int[n + 1]; for (int i = 1; i \u0026lt;= n; i++) { arr[i] = in.nextInt(); } System.out.println(maxMoney(1, n, arr)); } public static int maxMoney(int k, int n, int[] arr) { int left, right; // 获取左子树最大值，若没有左子树，则置为 0 if (2 * k \u0026lt;= n) left = maxMoney(2 * k, n, arr); else left = 0; // 获取右子树最大值，若没有左子树，则置为 0 if (2 * k + 1 \u0026lt;= n) right = maxMoney(2 * k + 1, n, arr); else right = 0; // 返回左右子树最大值与根节点之和 return Math.max(left, right) + arr[k]; } 参考文献 1.美团 | 后端笔试第二场 | 8/13\n","date":"2022-08-15T10:00:00+08:00","permalink":"https://jinggqu.github.io/posts/meituan-java-note-1/","title":"美团 Java 岗算法笔试记录（2022/08/13）"},{"content":"前言 LibTorch 简介 在 Python 深度学习圈，PyTorch 具有举足轻重的地位。同样的，C++ 平台上的 LibTorch 作为 PyTorch 的纯 C++ 接口，它遵循 PyTorch 的设计和架构，旨在支持高性能、低延迟的 C++ 深度学习应用研究。本文基于 Windows 环境与 Visual Studio 2019 开发工具，将从零开始搭建一个完整的深度学习开发环境，包括环境配置、项目演示、自定义数据集及问题排查等部分。\nLibTorch 安装 本文使用的 LibTorch 版本为 LTS(1.8.2) CPU 版，若需要使用 GPU 版，也可以在官方网站下载。\n环境配置 创建项目 首先，在 Visual Studio 中创建一个名为 libtorch-toturial 的控制台项目。创建完成后，将项目设置为 Release 模式，x64 平台，如下图。\n配置 LibTorch 依赖 本文中 LibTorch 解压后的存放目录为 D:\\Software\\libtorch-lts，后续配置过程中，读者请按照自己实际情况进行相关设置。\n在 Visual Studio 中，点击 项目 -\u0026gt; libtorch-toturial 项目属性，在左侧导航栏中找到 VC++ 目录 选项。在右侧的 包含目录 选项中将 LibTorch include 目录添加进去，详细如下。\n1 2 D:\\Software\\libtorch-lts\\include D:\\Software\\libtorch-lts\\include\\torch\\csrc\\api\\include 接着找到 库目录 选项，将 LibTorch lib 目录添加进去，详细如下。\n1 D:\\Software\\libtorch-lts\\lib 配置结果如下图，注意检查窗口顶栏 配置 是否为 Release，平台 是否为 x64。\n然后找到 链接器 -\u0026gt; 输入 -\u0026gt; 附加依赖项 选项，在其中填入 LibTorch lib 路径下（即 D:\\Software\\libtorch-lts\\lib）所有 *.lib 文件的文件名，详细如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 asmjit.lib c10.lib c10d.lib caffe2_detectron_ops.lib caffe2_module_test_dynamic.lib clog.lib cpuinfo.lib dnnl.lib fbgemm.lib fbjni.lib gloo.lib libprotobuf-lite.lib libprotobuf.lib libprotoc.lib mkldnn.lib pthreadpool.lib pytorch_jni.lib torch.lib torch_cpu.lib XNNPACK.lib 最后，将 D:\\Software\\libtorch-lts\\lib 路径下所有的 *.dll 文件拷贝至 项目路径 -\u0026gt; x64 -\u0026gt; Release 路径下，如下图。\n示例程序 至此，开发环境搭建就已经完成了。我们可以通过运行以下示例程序，来检验上述配置是否正确。若输出如图中所示，则配置无误。\n1 2 3 4 5 6 7 #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;iostream\u0026gt; auto main() -\u0026gt; int { auto array = torch::rand(10); std::cout \u0026lt;\u0026lt; array \u0026lt;\u0026lt; std::endl; } 手写数字识别 数据准备 本节将以深度学习经典案例——手写数字识别来演示 LibTorch 的使用。首先需要下载 mnist 手写数字数据集，你可以在这里下载，下载完成后将其解压到 libtorch-toturial.cpp 同一目录 data 文件夹下，目录结构如下。\n1 2 3 4 5 6 7 8 9 ├─libtorch-toturial │ │ libtorch-toturial.cpp │ │ ... │ ├─data │ │ t10k-images-idx3-ubyte │ │ t10k-labels-idx1-ubyte │ │ train-images-idx3-ubyte │ │ train-labels-idx1-ubyte │ ... 源代码 手写数字识别的源代码可以在 LibTorch 官方示例 中找到，请将其拷贝到项目的 libtorch-toturial.cpp 中。\n结果 与 PyTorch 类似，LibTorch 创建深度学习应用同样包含与其相似的步骤：定义网络、初始化网络、加载数据集、训练、验证及保存模型等，详细代码可以参照上述官方示例，此处不再赘述。训练 10 个 epoch 之后，识别准确率已经达到了 98.4%.\n自定义数据集 在本节中，我们将介绍如何将已有的数据集读取到神经网络中，生成 PyTorch 张量。在这之前，需要先介绍 NumCpp 工具，它可以大幅提升数据处理的效率。\nNumCpp 简介与配置 在 Python 开发环境中，最常用的工具非 NumPy 莫属，因其极为便捷高效的特性被开发者广为使用。同样的，在 C++ 平台上，也有开发者开发出了一款与 NumPy 体验“几乎一致”的 NumCpp ———— Python NumPy 库的模板头文件 C++ 实现[2]。\n由于 NumCpp 依赖 Boost 库，因此在配置 NumCpp 之前，需要先配置 Boost 库。相关文件可以在 Boost 官方网站 与 NumCpp Github 页面 进行下载。\n与 LibTorch 配置过程类似，我们需要在 Visual Studio 项目属性中找到 VC++ 目录 -\u0026gt; 包含目录 选项，将 Boost 库与 NumCpp 库的路径添加进去，具体路径如下。\n1 2 D:\\Software\\boost D:\\Software\\NumCpp\\include 然后即可使用下述程序片段进行检查是否配置正确，若成功运行并生成了 3x4 个浮点随机数，则说明配置无误。\n1 2 3 4 5 6 7 #include \u0026#34;NumCpp.hpp\u0026#34; #include \u0026lt;iostream\u0026gt; auto main() -\u0026gt; int { auto array = nc::random::randN\u0026lt;double\u0026gt;({ 3, 4 }); std::cout \u0026lt;\u0026lt; array \u0026lt;\u0026lt; std::endl; } 接下来可以使用 NumCpp 读取本地数据集，由于 NumCpp 缺少类似于 NumPy 的 loadtxt() 方法，故只能使用 fromfile()方法，具体代码如下。\n1 auto input_data = nc::fromfile\u0026lt;double\u0026gt;(input_filepath, /*sep=*/\u0026#39;,\u0026#39;); 假设数据实际尺寸为 m×n，读取到的数据形状为 1×(m×n)，所以还需要进行 reshape() 才可以正常使用。行切片与列切片也和 NumPy 类似，代码如下。\n1 2 3 4 5 6 7 input_data = input_data.reshape(m, n); // 行切片，形如 input_data = input_data[0:2, :] input_data = input_data(nc::Slice(0, 2), input_data.cSlice()); // 列切片，形如 input_data = input_data[:, :2] input_data = input_data(input_data.rSlice(), nc::Slice(0, 2)); 若要进行矩阵与矩阵的计算，则需要保证矩阵的尺寸一致。若不一致，则可以使用 tile() 方法进行扩充，示例代码如下。\n1 2 3 4 5 6 7 8 // 按列求均值，得到的矩阵为 1×n auto input_mean = nc::mean(input_data, nc::Axis::ROW); // 按列求标准差，得到的矩阵为 1×n auto input_std = nc::stdev(input_data, nc::Axis::ROW); // 归一化，将 input_mean 与 input_std 扩充为 m×n，再进行操作 input_data = (input_data - nc::tile(input_mean, { input_data.numRows(), 1 })) / nc::tile(input_std, { input_data.numRows(), 1 }); 自定义数据集 要实现自定义数据集，首先要继承 torch::data::Dataset\u0026lt;CustomDataset\u0026gt; 类，实现 CustomDataset() 构造方法、 get() 方法与 size() 方法。示例代码如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class CustomDataset : public torch::data::Dataset\u0026lt;CustomDataset\u0026gt; { private: std::vector\u0026lt;torch::Tensor\u0026gt; source, target; public: // 构造函数 CustomDataset(nc::NdArray\u0026lt;double\u0026gt; input_data, nc::NdArray\u0026lt;double\u0026gt; output_data, std::string data_type) { // 一些数据读取、处理工作。最后得到的 source 与 target 是输入与输出数据的集合 // 如果要对数据集进行划分，可以在此处声明一个方法进行详细处理 source = process_data(input_data, data_type); target = process_data(output_data, data_type); }; // 复写 get() 方法以返回第 index 个位置的张量（输入与输出） torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { torch::Tensor sample_source = source.at(index); torch::Tensor sample_target = target.at(index); return { sample_source.clone(), sample_target.clone() }; }; // 返回数据的数量 torch::optional\u0026lt;size_t\u0026gt; size() const override { return source.size(); }; }; 接下来调用 CustomDataset() 生成 data loader。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 训练数据 auto train_dataset = CustomDataset(input_data, output_data, \u0026#34;train_data\u0026#34;) .map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); const size_t train_dataset_size = train_dataset.size().value(); std::cout \u0026lt;\u0026lt; \u0026#34;train data size = \u0026#34; \u0026lt;\u0026lt; train_dataset_size \u0026lt;\u0026lt; std::endl; // 训练集 data loader auto train_loader = torch::data::make_data_loader(std::move(train_dataset), train_batch_size); // 验证数据 auto validate_dataset = CustomDataset(input_data, output_data, \u0026#34;validate_data\u0026#34;) .map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); const size_t validate_dataset_size = validate_dataset.size().value(); std::cout \u0026lt;\u0026lt; \u0026#34;validate data size = \u0026#34; \u0026lt;\u0026lt; validate_dataset_size \u0026lt;\u0026lt; std::endl; // 验证集 data loader auto validate_loader = torch::data::make_data_loader(std::move(validate_dataset), validate_batch_size); 与手写数字识别示例类似，在调用 train() 训练方法和 validate() 验证方法时，直接将 data loader 传入即可，代码示例如下。\n1 2 3 4 for (size_t epoch = 1; epoch \u0026lt;= kNumberOfEpochs; ++epoch) { train(epoch, model, device, *train_loader, optimizer, train_dataset_size); validate(model, device, *validate_loader, validate_dataset_size); } 疑难排查 网络浮点数精度 由于上述教程中使用 NumCpp 来读取数据，得到的数据集数据类型为泛型中指定的类型。LibTorch 网络初始化后的数据类型默认为 float(float32)，若我们读取的数据类型为 double(float64) 型，则需要手动将网络数据类型指定为 double，否则程序将会抛出异常[3]。\n1 2 Net model = Net(); model-\u0026gt;to(device, torch::kDouble); 模型保存再读取异常 当读取本地保存好的模型后，进行预测产生 loss 为 nan 的情况。经过 Debug 查看权重和张量数据，可以发现其均已经溢出了。这可能是由于保存的模型是 double 类型，而重新读取后初始化的模型为 float 类型，导致数据溢出。代码如下。\n1 2 3 4 5 6 7 8 9 10 Net model = Net(); model-\u0026gt;to(device, torch::kDouble); // 数据处理及网络训练与验证，并保存模型 torch::save(model, \u0026#34;test.pt\u0026#34;); Net new_model = Net(); // 首先将网络初始化为 double 类型 new_model-\u0026gt;to(device, torch::kDouble); // 从本地加载保存好的模型 torch::load(new_model, \u0026#34;test.pt\u0026#34;); C10 Error 如果在程序运行过程中抛出了 C10 Error，控制台也没有打印出错误信息，这是 LibTorch 一个已知的问题，详见参考文献[4]。为了得到实际的错误信息，此时我们可以使用 try catch 来手动捕获异常，代码如下。\n1 2 3 4 5 6 try { // 导致异常的代码块 } catch (std::exception \u0026amp;e) { std::cout \u0026lt;\u0026lt; e.what() \u0026lt;\u0026lt; std::endl; } 参考文献 LibTorch 教程 - Allent Dan NumCpp 官方文档 Does LibTorch not support float64 data training? After torch::load model and predict, then got NaN ","date":"2021-10-27T15:00:00+08:00","permalink":"https://jinggqu.github.io/posts/libtorch-toturial/","title":"LibTorch 上手教程"},{"content":"前言 在上一篇文章中，我们使用 Python 使用 SAE 网络实现了手写数字的重建。在本文中，我们将尝试使用 tiny_dnn 库实现手写数字重建。\ntiny_dnn 简介 tiny-dnn 项目地址：https://github.com/tiny-dnn/tiny-dnn，这是深度学习的一个 C ++ 14 实现。它适合在有限的计算资源，嵌入式系统和 IoT 设备上进行深度学习。整个项目仅由头文件构成，使用时无需编译，直接引用即可。\n搭建环境 版本要求 需要一个 C++ 14 编译器，例如 gcc 4.9+，clang 3.6+ 或者 VS 2015+。本文中使用 Visual Studio 2019 为例进行配置。\n创建项目 打开 VS，创建一个名为 testTinyDNN 的控制台应用。将 tiny_dnn 下载解压之后，放置到如下图所示的位置，与 testTinyDNN.cpp 属于同一层级。\n编辑配置 编辑 config.h 文件第 61 行，将其取消注释；这样我们才可以将栈式自编码器预测的图片保存到本地。涉及内容如下： 1 2 3 4 5 /** * Enable Image API support. * Currently we use stb by default. **/ #define DNN_USE_IMAGE_API 编辑 image.h 文件第 378 行，将 border_width 值设置为 0，这样保存的图片每个像素周围就不会存在白色边框。涉及内容如下： 1 const size_t border_width = 0; 编写代码 打开 testTinyDNN.cpp 文件，将下列代码粘贴进去。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026#34;tiny_dnn/tiny_dnn.h\u0026#34; using namespace tiny_dnn; using namespace tiny_dnn::activation; using namespace tiny_dnn::layers; using namespace std; #define EPOCHS 50 #define BATCH_SIZE 256 void sae() { // define network, optimizer and engine network\u0026lt;sequential\u0026gt; net; adam optimizer; core::backend_t backend_type = core::default_engine(); // construct network layers, include 3 encoder layers and 3 decoder layers net \u0026lt;\u0026lt; fully_connected_layer(784, 128, true, backend_type) \u0026lt;\u0026lt; relu() \u0026lt;\u0026lt; fully_connected_layer(128, 64, true, backend_type) \u0026lt;\u0026lt; relu() \u0026lt;\u0026lt; fully_connected_layer(64, 32, true, backend_type) \u0026lt;\u0026lt; relu() \u0026lt;\u0026lt; fully_connected_layer(32, 64, true, backend_type) \u0026lt;\u0026lt; relu() \u0026lt;\u0026lt; fully_connected_layer(64, 128, true, backend_type) \u0026lt;\u0026lt; sigmoid() \u0026lt;\u0026lt; fully_connected_layer(128, 784, true, backend_type); // load MNIST dataset vector\u0026lt;vec_t\u0026gt; train_images, test_images; string data_dir_path = \u0026#34;tiny_dnn/data\u0026#34;; parse_mnist_images(data_dir_path + \u0026#34;/train-images.idx3-ubyte\u0026#34;, \u0026amp;train_images, -1.0, 1.0, 0, 0); parse_mnist_images(data_dir_path + \u0026#34;/t10k-images.idx3-ubyte\u0026#34;, \u0026amp;test_images, -1.0, 1.0, 0, 0); cout \u0026lt;\u0026lt; \u0026#34;start training\u0026#34; \u0026lt;\u0026lt; endl; // define learning rate (0.05) optimizer.alpha *= static_cast\u0026lt;tiny_dnn::float_t\u0026gt;(0.05); // display training progress bar, and show training duration progress_display disp(static_cast\u0026lt;unsigned long\u0026gt;(train_images.size())); timer t; // create callback int epoch = 0; auto on_enumerate_epoch = [\u0026amp;]() { epoch++; cout \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34; \u0026lt;\u0026lt; t.elapsed() \u0026lt;\u0026lt; \u0026#34;s elapsed.\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;epoch=\u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; \u0026#34;/\u0026#34; \u0026lt;\u0026lt; EPOCHS \u0026lt;\u0026lt; endl; disp.restart(static_cast\u0026lt;unsigned long\u0026gt;(train_images.size())); t.restart(); }; auto on_enumerate_minibatch = [\u0026amp;]() { disp += BATCH_SIZE; }; // training net.fit\u0026lt;mse\u0026gt;(optimizer, train_images, train_images, BATCH_SIZE, EPOCHS, on_enumerate_minibatch, on_enumerate_epoch); // save model net.save(\u0026#34;sae-net\u0026#34;); cout \u0026lt;\u0026lt; \u0026#34;end training.\u0026#34; \u0026lt;\u0026lt; endl; // if the model already exists, you can read it directly //net.load(\u0026#34;sae-net\u0026#34;); // save layers to image //for (size_t i = 0; i \u0026lt; net.depth(); i++) { // auto out_img = net[i]-\u0026gt;output_to_image(); // auto filename = \u0026#34;layer_\u0026#34; + to_string(i) + \u0026#34;.bmp\u0026#34;; // out_img.save(filename); //} // test and show results for (int i = 0; i \u0026lt; 10; i++) { // get predicted result image auto predict = net.predict(test_images[i]); // save predicted result image to file auto image = vec2image\u0026lt;float\u0026gt;(predict, 10, 28); auto filename = \u0026#34;image_predicted_\u0026#34; + to_string(i) + \u0026#34;.bmp\u0026#34;; image.save(filename); // save the origin test image to file image = vec2image\u0026lt;float\u0026gt;(test_images[i], 10, 28); filename = \u0026#34;image_test_\u0026#34; + to_string(i) + \u0026#34;.bmp\u0026#34;; image.save(filename); } } int main() { sae(); } 在代码中，我们定义了每批次训练数据量为 256 条，总共训练 50 个批次。\n网络结构为 3 个编码层 + 3 个解码层。编码层将数据从 784（28 * 28） 维分别编码（降维）到 128、64、32 维，解码器再将 32 维的编码结果解码（升维）到 64、128、784 维，完成手写数字重建。各层之间的激活函数选用 relu() 与 sigmoid()。\n结果展示 从上到下，第一行为测试图像，第二行为 keras 搭建的 SAE 网络重建图像，第三行为 tiny_dnn 搭建的 SAE 网络重建图像。下面展示数字 2 和 5 重建的详细效果，左侧为 Python 平台重建结果，右侧为 C++ 平台重建结果。\n数字 2\n数字 5\n性能对比 测试使用的 CPU 型号为 Intel i5-4200H，基准频率为 2.80GHz。\n基于 tiny_dnn 的 C++ 平台训练时长为 2624.95 秒，基于 keras 的 Python 平台训练时长为 135.70 秒。在 50 个 epoch 测试中，Python 平台比 C++ 平台快了大约 19 倍，Python 平台 loss 大约为 0.08。由重建图片结果不难看出，Python 平台效果明显优于 C++ 平台。\n存在的不足 C++ 平台目前无法计算每个 epoch 的 loss； 将在 C++ 平台测试更多的 epoch，观察图像重建效果是否会有改善。 参考文献 A simple and basic tutorial of tiny-dnn A quick introduction to tiny-dnn Details about tiny-dnn’s API and short examples ","date":"2021-01-28T18:00:00+08:00","permalink":"https://jinggqu.github.io/posts/sae-2/","title":"SAE 入门（二）——基于 tiny_dnn 的手写数字重建"},{"content":"Autoencoder 简介 自编码器（Autoencoder，AE），是一种利用反向传播（backpropagation，BP）算法使得输出值等于输入值的神经网络，它先将输入压缩成潜在空间表征，然后通过这种表征来重构输出。其中，空间表征可以看作是输入数据的高级抽象，通常是将高维度的数据抽象为低维度的数据。\n自编码器由两部分组成：\n编码器：这部分能将输入压缩成潜在空间表征，可以用编码函数 $h=f(x)$ 表示;\n解码器：这部分能重构来自潜在空间表征的输入，可以用解码函数 $r=g(h)$ 表示。\n因此，整个自编码器可以用函数 $g(f(x)) = r$ 来描述，其中输出 $r$ 与原始输入 $x$ 相近。\n自动编码器的目标是最大程度地减少输入和输出之间的重构误差。这有助于自动编码器学习数据中存在的重要功能。当表征很好地重建其输入时，则表示这个表征很好地保留了输入中存在的许多信息。整个过程如下图。\nStacked Autoencoder 简介 Stacked Autoencoder 简写作 SAE。SAE 与 AE 的主要区别在于编码器与解码器的层数，栈式自编码器包含多层隐藏层。具体网络结构如下图所示，图中有两层编码层，两层解码层。\n代码实现 代码环境配置，请参考 GAN 网络之手写数字生成 第一小节——环境搭建。\n自编码器只是一种思想，在具体实现中，编码器和解码器可以由多种深度学习模型构成，例如全连接层、卷积层和 LSTM 等，以下使用 Keras 来实现栈式自编码器。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 from keras.datasets import mnist from keras.layers import Input, Dense from keras.models import Model import numpy as np import matplotlib.pyplot as plt EPOCHS = 50 BATCH_SIZE = 256 def train(x_train, x_test): input_img = Input(shape=(784,)) # 三个编码层，将数据从 784 维向量编码为 128、64、32 维向量 encoded = Dense(units=128, activation=\u0026#39;relu\u0026#39;)(input_img) encoded = Dense(units=64, activation=\u0026#39;relu\u0026#39;)(encoded) encoded = Dense(units=32, activation=\u0026#39;relu\u0026#39;)(encoded) # 三个解码层，将数据从 32 维向量解码成 64、128、784 维向量 decoded = Dense(units=64, activation=\u0026#39;relu\u0026#39;)(encoded) decoded = Dense(units=128, activation=\u0026#39;relu\u0026#39;)(decoded) decoded = Dense(units=784, activation=\u0026#39;sigmoid\u0026#39;)(decoded) autoencoder = Model(input_img, decoded) encoder = Model(input_img, encoded) autoencoder.summary() encoder.summary() autoencoder.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) autoencoder.fit(x_train, x_train, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(x_test, x_test)) return encoder, autoencoder def plot(encoded_imgs, decoded_imgs): plt.figure(figsize=(40, 4)) for i in range(10): # 展示原始输入图像 ax = plt.subplot(3, 20, i + 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # 展示编码后的图像 ax = plt.subplot(3, 20, i + 1 + 20) plt.imshow(encoded_imgs[i].reshape(8, 4)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # 展示解码后的输入图像 ax = plt.subplot(3, 20, 2 * 20 + i + 1) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() if __name__ == \u0026#39;__main__\u0026#39;: # 加载数据，训练数据 60000 条，测试数据 10000 条，数据灰度值 [0, 255] (x_train, _), (x_test, _) = mnist.load_data() # 正则化数据，将灰度值区间转换为 [0, 1] x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255 x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255 # 将数据集从二维 (28, 28) 矩阵转换为长度为维度是 784 的向量 x_train = x_train.reshape(len(x_train), np.prod(x_train.shape[1:])) x_test = x_test.reshape(len(x_test), np.prod(x_test.shape[1:])) print(x_train.shape) print(x_test.shape) # 训练数据 encoder, autoencoder = train(x_train, x_test) # 获取编码后和解码后的图像 encoded_imgs = encoder.predict(x_test) decoded_imgs = autoencoder.predict(x_test) # 绘制图像 plot(encoded_imgs, decoded_imgs) 运行上述代码，可以从输出内容中得到以下信息：\n输入数据是 60000 张手写数字的灰度图像，灰度取值范围是 [0, 255]，我们将其灰度值按行依次存储到一个 1 * 784 的数组中； 输入数据形如 (0, 0, 0,\u0026hellip;, 84, 185, 159,\u0026hellip;, 170, 52,\u0026hellip;, 0, 0)，我们可以将每张图片（每个向量）理解为一个 784 维空间的中向量； 通过正则化后，输入数据每个维度区间变为 [0, 1]； 编码层将输入的 784 维向量抽象为 128、64、32 维向量（dense，dense_1，dense_2）； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Model: \u0026#34;functional_3\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 784)] 0 _________________________________________________________________ dense (Dense) (None, 128) 100480 _________________________________________________________________ dense_1 (Dense) (None, 64) 8256 _________________________________________________________________ dense_2 (Dense) (None, 32) 2080 ================================================================= Total params: 110,816 Trainable params: 110,816 Non-trainable params: 0 _________________________________________________________________ 解码层将抽象后的 32 维向量还原维 64、128、784 维向量（dense_3，dense_4，dense_5）； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Model: \u0026#34;functional_1\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 784)] 0 _________________________________________________________________ dense (Dense) (None, 128) 100480 _________________________________________________________________ dense_1 (Dense) (None, 64) 8256 _________________________________________________________________ dense_2 (Dense) (None, 32) 2080 _________________________________________________________________ dense_3 (Dense) (None, 64) 2112 _________________________________________________________________ dense_4 (Dense) (None, 128) 8320 _________________________________________________________________ dense_5 (Dense) (None, 784) 101136 ================================================================= Total params: 222,384 Trainable params: 222,384 Non-trainable params: 0 _________________________________________________________________ 训练完成之后，可以在输出内容中看到详细的训练数据，在 50 次训练之后，loss 已经降低到了 0.08。得到的输出图像如下图所示。\n1 2 3 4 5 6 7 8 ...... Epoch 48/50 235/235 [==============================] - 3s 12ms/step - loss: 0.0848 - accuracy: 0.0130 - val_loss: 0.0844 - val_accuracy: 0.0147 Epoch 49/50 235/235 [==============================] - 3s 12ms/step - loss: 0.0846 - accuracy: 0.0130 - val_loss: 0.0845 - val_accuracy: 0.0115 Epoch 50/50 235/235 [==============================] - 3s 12ms/step - loss: 0.0845 - accuracy: 0.0139 - val_loss: 0.0840 - val_accuracy: 0.0165 参考文献 Sparse, Stacked and Variational Autoencoder Deep Learning Autoencoders Deep Autoencoder using Keras 自编码器是什么？有什么用？这里有一份入门指南（附代码） 反向传播算法 - 维基百科 Autoencoder - Github ","date":"2021-01-20T18:00:00+08:00","permalink":"https://jinggqu.github.io/posts/sae-1/","title":"SAE 入门（一）"},{"content":"环境搭建 本例中，所涉及的系统与软件版本列表如下。\n名称 版本 操作系统 Windows 20H2 Anaconda Anaconda3-2020.11 python 3.6 tensorflow 1.8.0 本例代码存放于 https://github.com/jinggqu/MachineLearning。\nAnaconda 安装 通过清华大学开源软件镜像站，我们可以直接下载最新版本的 Anaconda，本例中使用的 Anaconda 下载链接： https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2020.11-Windows-x86_64.exe\nAnaconda 安装教程网络上已经有很多，故此处不再赘述。\n安装完成后，我们需要手动配置 Anaconda 的环境变量，在用户变量的 Path 中添加 Anaconda 的安装路径以及其子文件夹，具体内容如下。\n1 2 3 C:\\Users\\xvyn\\anaconda3 C:\\Users\\xvyn\\anaconda3\\Scripts C:\\Users\\xvyn\\anaconda3\\Library\\bin 上述配置请根据 Anaconda 实际安装路径进行调整，配置完成的效果如下图所示。\n完成后打开 cmd 输入下列命令，如果输出内容与下列内容类似，则表示配置正确，可继续后面的步骤。\n1 conda --version 输出 conda 4.9.2 创建虚拟环境 通过如下命令进行创建一个虚拟环境。\n1 conda create -n handwrittendigits -n handwrittendigits 的作用是指定虚拟环境的名称，本例中指定为 handwrittendigits。\n执行结束后，可通过下列命令查看 Anaconda 中所有的虚拟环境。\n1 conda info --evns 输出如下\n(base) PS C:\\Users\\xvyn\u0026gt; conda info --envs conda environments: base * C:\\Users\\xvyn\\anaconda3 handwrittendigits C:\\Users\\xvyn\\anaconda3\\envs\\handwrittendigits 其中，标记 * 的表示目前已启用，命令行前半部分的 (base) 也表示目前启用的是哪个虚拟环境，此例中为 base 环境。\n切换虚拟环境 如果使用 PowerShell 进行 Anaconda 的一些操作，需要以 管理员 身份运行 PowerShell，然后执行下列命令。\n1 set-executionpolicy remotesigned 执行完成后可通过下列命令进行切换虚拟环境。若使用其他 Shell 工具进行操作，则可直接执行下列命令。\n1 conda activate handwrittendigits 如果执行时报错如下，则可以通过 https://github.com/conda/conda/issues/7980 来解决。\nCan't execute `conda activate` from batch script 详细操作为：\n安装并打开 Git Bash 执行 source ~/anaconda3/etc/profile.d/conda.sh 执行 conda init 重启 PowerShell 切换环境操作结束后，可以注意到命令行左侧的括号内容由 (base) 变为 (handwrittendigits)，表明切换成功，后面的操作均在此虚拟环境中进行。\n实际操作过程 (base) PS C:\\Users\\xvyn\u0026gt; conda activate handwrittendigits (handwrittendigits) PS C:\\Users\\xvyn\u0026gt; 再次查看所有虚拟环境 (handwrittendigits) PS C:\\Users\\xvyn\u0026gt; conda info --envs conda environments: base C:\\Users\\xvyn\\anaconda3 handwrittendigits * C:\\Users\\xvyn\\anaconda3\\envs\\handwrittendigits 更换镜像源（不推荐） 由于 Anaconda 和 pip 官方镜像源访问缓慢，故需要将镜像源更换为国内镜像源，例如清华大学、中科大与阿里云镜像源。使用下列命令可以查看当前 Anaconda 镜像源。\n1 conda config --show 在输出中找到 channel 部分，有如下内容。\nchannels: - defaults default_channels: - https://repo.anaconda.com/pkgs/main - https://repo.anaconda.com/pkgs/r - https://repo.anaconda.com/pkgs/msys2 更换 Anaconda 镜像源 以清华大学镜像源为例，执行下列命令即可完成更换。\n1 2 3 4 5 6 7 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/ conda config --set show_channel_urls yes 恢复默认源\n1 conda config --remove-key channels 除了上述命令行操作方式外，也可以直接修改 C:\\Users\u0026lt;USER\u0026gt;\\.condarc 文件来实现换源。参考 Anaconda 镜像使用帮助 修改后的文件内容如下所示。\nssl_verify: false show_channel_urls: true channels: - defaults default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 更换 pip 镜像源（不推荐） 以清华大学镜像源为例，执行下列命令即可完成更换。\n1 pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple 安装 tensorflow 手写数字生成例子所需要的 tensorflow 版本为 1.x，本例中我们使用的实际版本为 1.8.0。将虚拟环境切换到 handwrittendigits 后，执行以下命令开始安装。\n1 conda install tensorflow-gpu=1.8.0 上述命令中 tensorflow-gpu 表示安装的 tensorflow 为 GPU 版本，=1.8.0 指定了安装的版本号。若需要安装 CPU 版 tensorflow 1.8.0，执行以下命令即可。\n1 conda install tensorflow=1.8.0 安装 Python 由于需要 1.8.0 版本的 tensorflow，此版本仅兼容 3.5 到 3.7 版本的 Python，故需要先删除 conda 环境中默认安装的 Python，并安装 3.6 版本。\n1 2 3 4 5 # 移除自带 Python conda remove python # 安装 3.6 版本 conda install python=3.6 测试 Demo 使用 PyCharm 创建项目 在创建项目时，需要将虚拟环境（图中 Location 项）配置为前文中创建的虚拟环境所在目录，然后点击创建项目。\n由于此前作者已经创建过项目，故创建窗口下方会提示虚拟环境目录不为空，忽略即可。\n运行项目 将以下代码置于项目 main.py 中，运行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import logging import os if not os.path.exists(\u0026#39;./log\u0026#39;): os.mkdir(\u0026#39;./log\u0026#39;) if not os.path.exists(\u0026#39;./out\u0026#39;): os.mkdir(\u0026#39;./out\u0026#39;) def get_logger(filepath, level=logging.INFO): logger = logging.getLogger(__name__) logger.setLevel(level) # create a file handler handler = logging.FileHandler(filepath) handler.setLevel(logging.INFO) # create a logging format # formatter = logging.Formatter(\u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39;) # handler.setFormatter(formatter) # add the handlers to the logger logger.addHandler(handler) return logger def plot(samples): fig = plt.figure(figsize=(4, 4)) gs = gridspec.GridSpec(4, 4) gs.update(wspace=0.05, hspace=0.05) for i, sample in enumerate(samples): ax = plt.subplot(gs[i]) plt.axis(\u0026#39;off\u0026#39;) ax.set_xticklabels([]) ax.set_yticklabels([]) ax.set_aspect(\u0026#39;equal\u0026#39;) plt.imshow(sample.reshape(28, 28), cmap=\u0026#39;Greys_r\u0026#39;) return fig def random_data(row, column): return np.random.uniform(-1., 1., size=[row, column]) def weight_variable(shape, stddev=0.1): initial = tf.truncated_normal(shape, stddev=stddev) return tf.Variable(initial) def bias_variable(shape, bais=0.1): initial = tf.constant(bais, shape=shape) return tf.Variable(initial) # 鉴别网络weights d_w1 = weight_variable([784, 128]) d_b1 = bias_variable([128]) d_w2 = weight_variable([128, 1]) d_b2 = bias_variable([1]) param_d = [d_w1, d_w2, d_b1, d_b2] # 生成网络weights g_w1 = weight_variable([100, 128]) g_b1 = bias_variable([128]) g_w2 = weight_variable([128, 784]) g_b2 = bias_variable([784]) param_g = [g_w1, g_w2, g_b1, g_b2] # 鉴别网络 def d_network(x): d1 = tf.nn.relu(tf.matmul(x, d_w1) + d_b1) d_out = tf.matmul(d1, d_w2) + d_b2 return tf.nn.sigmoid(d_out) # 生成网络 def g_network(x): g1 = tf.nn.relu(tf.matmul(x, g_w1) + g_b1) g_out = tf.matmul(g1, g_w2) + g_b2 return tf.nn.sigmoid(g_out) x = tf.placeholder(tf.float32, shape=[None, 784]) z = tf.placeholder(tf.float32, shape=[None, 100]) d_out_real = d_network(x) g_out = g_network(z) d_out_fake = d_network(g_out) d_loss = -tf.reduce_mean(tf.log(d_out_real) + tf.log(1. - d_out_fake)) g_loss = -tf.reduce_mean(tf.log(d_out_fake)) d_optimizer = tf.train.AdamOptimizer().minimize(d_loss, var_list=param_d) g_optimizer = tf.train.AdamOptimizer().minimize(g_loss, var_list=param_g) batch_size = 256 max_step = 1000000 mnist = input_data.read_data_sets(\u0026#39;../mnist\u0026#39;, one_hot=True) logger = get_logger(\u0026#34;./log/info.log\u0026#34;) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(\u0026#34;training\u0026#34;) i = 0 for step in range(max_step): batch_real, _ = mnist.train.next_batch(batch_size) _, d_loss_train = sess.run([d_optimizer, d_loss], feed_dict={x: batch_real, z: random_data(batch_size, 100)}) _, g_loss_train = sess.run([g_optimizer, g_loss], feed_dict={z: random_data(batch_size, 100)}) if step % 1000 == 0: samples = sess.run(g_out, feed_dict={z: random_data(16, 100)}) fig = plot(samples) plt.savefig(\u0026#39;out/{}.png\u0026#39;.format(str(i).zfill(4)), bbox_inches=\u0026#39;tight\u0026#39;) i += 1 plt.close(fig) logger.info(\u0026#34;step %s: d_loss is %s, gan_loss is %s\u0026#34; % (step, d_loss_train, g_loss_train)) print(\u0026#34;step %s: d_loss is %s, g_loss is %s\u0026#34; % (step, d_loss_train, g_loss_train)) 运行时的截图如下，可以看到已经生成了多张手写数字的图片。\n至此，GAN 网络手写数字生成环境搭建已经完成，后续将进行更加深入的学习。\n备注 为 jupyter lab 指定 conda 环境，在 conda 环境中执行以下命令后再启动 jupyter lab 即可\n1 conda install nb_conda 参考文章 Anaconda 源使用帮助 gan_practice Can\u0026rsquo;t execute conda activate from bash script python 安装 TensorFlow 吐血整理 conda 安装指定版本的指定包 Python pip 命令行设置国内镜像源 ","date":"2020-12-08T10:00:00+08:00","permalink":"https://jinggqu.github.io/posts/gan-for-hand-written-digits/","title":"GAN 网络之手写数字生成"},{"content":" ……我细读来书，终觉得你不免作茧自缚。你自己去寻出一个本不成问题的问题，“人生有何意义？”其实这个问题是容易解答的。人生的意义全是各人自己寻出来、造出来的：高尚、卑劣、清贵、污浊、有用、无用，……全靠自己的作为。\n生命本身不过是一件生物学的事实，有什么意义可说？一个人与一只猎，一只狗，有什么分别？人生的意义不在于何以有生，而在自己怎样生活。你若情愿把这六尺之躯葬送在白昼作梦之上二那就是你这一生的意义。你若发愤振作起来，决心去寻求生命的意义，去创造自己的生命的意义，那么，你活一日便有一日的意义，作一事便添一事的意义，生命无穷，生命的意义也无穷了。\n总之，生命本没有意义，你要能给他什么意义，他就有什么意义。与其终日冥想人生有何意义，不如试用此生作点有意义的事……\n节选自《答某君书》—— 胡适\n","date":"2019-06-21T00:00:00+08:00","permalink":"https://jinggqu.github.io/posts/the-meaning-of-life/","title":"The meaning of life"}]